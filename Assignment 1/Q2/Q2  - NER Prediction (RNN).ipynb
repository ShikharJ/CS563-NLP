{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries.\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import Model, Sequential\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import *\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "random.seed(11)\n",
    "np.random.seed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NER-Dataset-10Types-Train.txt\t NER-TestSet-10Types-RNN-Predictions.txt\r\n",
      " NER-Dataset--TestSet.txt\t'Q2 - NER Prediction - 10 Types.ipynb'\r\n",
      " NER-Dataset-Train.txt\t\t'Q2  - NER Prediction.ipynb'\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NER-Dataset-Train.txt', 'r') as f:\n",
    "    ner_dataset = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "words = []\n",
    "tags = []\n",
    "for line in ner_dataset:\n",
    "    line = line.strip()\n",
    "    if line == '':\n",
    "        sentences.append((words, tags))\n",
    "        words = []\n",
    "        tags = []\n",
    "    else:\n",
    "        word, tag = line.split('\\t')\n",
    "        words.append(word)\n",
    "        tags.append(tag)\n",
    "\n",
    "if len(words) > 0:\n",
    "    sentences.append((words, tags))\n",
    "    words = []\n",
    "    tags= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_counts = Counter(sum([a[0] for a in sentences], [])).most_common()\n",
    "words_to_keep = set([word for word, count in vocab_counts if count > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NER-Dataset--TestSet.txt', 'r') as f:\n",
    "    test_dataset = f.readlines()\n",
    "\n",
    "test_sentences = []\n",
    "words = []\n",
    "for line in test_dataset:\n",
    "    line = line.strip()\n",
    "    if line == '':\n",
    "        test_sentences.append((words,))\n",
    "        words = []\n",
    "    else:\n",
    "        word = line\n",
    "        words.append(word)\n",
    "\n",
    "if len(words) > 0:\n",
    "    test_sentences.append((words,))\n",
    "    words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = ['twoDigitNum',\n",
    "                'fourDigitNum',\n",
    "                'containsDigitAndAlpha',\n",
    "                'containsDigitAndDash',\n",
    "                'containsDigitAndSlash',\n",
    "                'containsDigitAndComma',\n",
    "                'containsDigitAndPeriod',\n",
    "                'otherNum',\n",
    "                'allCaps',\n",
    "                'capPeriod',\n",
    "                'firstWord',\n",
    "                'initCap',\n",
    "                'lowerCase',\n",
    "                'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_features(sentence):\n",
    "    features = []\n",
    "    ## Optimize and use an Enum!\n",
    "    firstword = True\n",
    "    for word in sentence:\n",
    "        if word.isnumeric() and len(word) == 2:\n",
    "            features.append('twoDigitNum')\n",
    "        elif word.isnumeric() and len(word) == 4:\n",
    "            features.append('fourDigitNum')\n",
    "        elif word.isalnum() and not word.isalpha() and not word.isnumeric():\n",
    "            features.append('containsDigitAndAlpha')\n",
    "        elif word.replace('-', '').isnumeric():\n",
    "            features.append('containsDigitAndAlpha')\n",
    "        elif word.replace('/', '').isnumeric():\n",
    "            features.append('containsDigitAndSlash')\n",
    "        elif word.replace('.', '').replace(',', '').isnumeric() and ',' in word:\n",
    "            features.append('containsDigitAndComma')\n",
    "        elif word.replace('.', '').isnumeric():\n",
    "            features.append('containsDigitAndPeriod')\n",
    "        elif word.isnumeric():\n",
    "            features.append('otherNum')\n",
    "        elif word.isupper():\n",
    "            features.append('allCaps')\n",
    "        elif len(word) == 2 and word[0].isupper() and word[1] == '.':\n",
    "            features.append('capPeriod')\n",
    "        elif firstword:\n",
    "            features.append('firstWord')\n",
    "        elif word[0].isupper():\n",
    "            features.append('initCap')\n",
    "        elif word.islower():\n",
    "            features.append('lowerCase')\n",
    "        else:\n",
    "            features.append('other')\n",
    "        firstword = False\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_found = max(len(s[0]) for s in sentences)\n",
    "max_len = max_len_found + ((50 - (max_len_found % 50)) % 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_mat = list(np.eye(len(word_features)))\n",
    "wordfeat2float = {feat: eye_mat[i] for i, feat in enumerate(word_features)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2idx = {'UNK': 0, 'PAD': 1}\n",
    "word2idx.update({word: i + 2 for i, word in enumerate(sorted(words_to_keep))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberize_sentence(words, max_len=50):\n",
    "    features = get_word_features(words)\n",
    "    word_idx = [word2idx[w] if w in word2idx.keys() else word2idx['UNK'] for w in words]\n",
    "    feat_np = [wordfeat2float[f] for f in features]\n",
    "    word_padding = [word2idx['PAD'] for _ in range(max_len - len(word_idx))]\n",
    "    feat_padding = [np.ones((len(word_features),)) * 2 for _ in range(max_len - len(word_idx))]\n",
    "    word_idx = np.asarray(word_idx + word_padding)\n",
    "    feat_np = np.asarray(feat_np + feat_padding)\n",
    "    return word_idx, feat_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = set.union(*(set(s[1]) for s in sentences))\n",
    "idx2labels = {i: s for i, s in enumerate(labels)}\n",
    "n_labels = len(labels)\n",
    "eye_mat = list(np.eye(len(labels)))\n",
    "labels2float = {feat: eye_mat[i] for i, feat in enumerate(labels)}\n",
    "\n",
    "def numberize_labels(gt_labels, max_len=50):\n",
    "    labels_np = [labels2float[l] for l in gt_labels]\n",
    "    labels_padding = [labels2float['O'] for _ in range(max_len - len(gt_labels))]\n",
    "    return np.asarray(labels_np + labels_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    input_words = Input(shape = (max_len,))\n",
    "    input_feats = Input(shape = (max_len, len(word_features)))\n",
    "    masked_words = Masking(mask_value = 1)(input_words)\n",
    "    masked_feats = Masking(mask_value = 2)(input_feats)\n",
    "    emb = Embedding(input_dim = (len(word2idx)), output_dim = 50, input_length = max_len)(masked_words)\n",
    "    drop_emb = Dropout(0.1)(emb)\n",
    "    concat_out = Concatenate()([drop_emb, masked_feats])\n",
    "    rnn_out = Bidirectional(SimpleRNN(units = 100, return_sequences = True, recurrent_dropout = 0.1))(concat_out)\n",
    "    dense_out = TimeDistributed(Dense(n_labels, activation = \"softmax\"))(rnn_out)\n",
    "    model = Model(inputs = [input_words, input_feats], outputs = dense_out)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_sentences = [(numberize_sentence(s[0]), numberize_labels(s[1])) for s in sentences]\n",
    "parsed_test_sentences = [numberize_sentence(s[0]) for s in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 44028, 2: 582, 0: 390})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(sum([s[1] for s in sentences], []))\n",
    "Counter(sum([np.argmax(s[1], axis=-1).tolist() for s in parsed_sentences], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 50)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 50, 50)       72900       masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 50, 14)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 50, 50)       0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, 50, 14)       0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 50, 64)       0           dropout_1[0][0]                  \n",
      "                                                                 masking_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 50, 200)      33000       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 50, 3)        603         bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 106,503\n",
      "Trainable params: 106,503\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shikhar/anaconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 144 samples\n",
      "Epoch 1/3\n",
      "576/576 [==============================] - 15s 26ms/step - loss: 0.0950 - accuracy: 0.9346 - val_loss: 0.0667 - val_accuracy: 0.9485\n",
      "Epoch 2/3\n",
      "576/576 [==============================] - 14s 24ms/step - loss: 0.0619 - accuracy: 0.9475 - val_loss: 0.0603 - val_accuracy: 0.9506\n",
      "Epoch 3/3\n",
      "576/576 [==============================] - 13s 22ms/step - loss: 0.0512 - accuracy: 0.9556 - val_loss: 0.0615 - val_accuracy: 0.9478\n",
      "[still updating...] Accuracy: 0.949859943977591, Precision: 0.6654204992439212, Recall: 0.5902111321273259, FScore: 0.6223229328172685\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_3 (Masking)             (None, 50)           0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 50, 50)       72900       masking_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 50, 14)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 50, 50)       0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "masking_4 (Masking)             (None, 50, 14)       0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 50, 64)       0           dropout_2[0][0]                  \n",
      "                                                                 masking_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 50, 200)      33000       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 50, 3)        603         bidirectional_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 106,503\n",
      "Trainable params: 106,503\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shikhar/anaconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 144 samples\n",
      "Epoch 1/3\n",
      "576/576 [==============================] - 17s 29ms/step - loss: 0.0895 - accuracy: 0.9420 - val_loss: 0.0694 - val_accuracy: 0.9435\n",
      "Epoch 2/3\n",
      "576/576 [==============================] - 18s 31ms/step - loss: 0.0627 - accuracy: 0.9481 - val_loss: 0.0582 - val_accuracy: 0.9535\n",
      "Epoch 3/3\n",
      "576/576 [==============================] - 18s 31ms/step - loss: 0.0522 - accuracy: 0.9573 - val_loss: 0.0567 - val_accuracy: 0.9528\n",
      "[still updating...] Accuracy: 0.9489781536293164, Precision: 0.7275867773129755, Recall: 0.5541793045701331, FScore: 0.6144008766626959\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_5 (Masking)             (None, 50)           0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 50, 50)       72900       masking_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 50, 14)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 50, 50)       0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "masking_6 (Masking)             (None, 50, 14)       0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 50, 64)       0           dropout_3[0][0]                  \n",
      "                                                                 masking_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 50, 200)      33000       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 50, 3)        603         bidirectional_3[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 106,503\n",
      "Trainable params: 106,503\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shikhar/anaconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 144 samples\n",
      "Epoch 1/3\n",
      "576/576 [==============================] - 16s 28ms/step - loss: 0.0897 - accuracy: 0.9382 - val_loss: 0.0738 - val_accuracy: 0.9446\n",
      "Epoch 2/3\n",
      "576/576 [==============================] - 23s 40ms/step - loss: 0.0620 - accuracy: 0.9502 - val_loss: 0.0645 - val_accuracy: 0.9410\n",
      "Epoch 3/3\n",
      "576/576 [==============================] - 26s 44ms/step - loss: 0.0512 - accuracy: 0.9570 - val_loss: 0.0575 - val_accuracy: 0.9546\n",
      "[still updating...] Accuracy: 0.9501140250855188, Precision: 0.7463736340760093, Recall: 0.5559133901490454, FScore: 0.620367438437964\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_7 (Masking)             (None, 50)           0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 50, 50)       72900       masking_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 50, 14)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 50, 50)       0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "masking_8 (Masking)             (None, 50, 14)       0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 50, 64)       0           dropout_4[0][0]                  \n",
      "                                                                 masking_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 50, 200)      33000       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 50, 3)        603         bidirectional_4[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 106,503\n",
      "Trainable params: 106,503\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shikhar/anaconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 144 samples\n",
      "Epoch 1/3\n",
      "576/576 [==============================] - 24s 42ms/step - loss: 0.0959 - accuracy: 0.9334 - val_loss: 0.0664 - val_accuracy: 0.9499\n",
      "Epoch 2/3\n",
      "576/576 [==============================] - 24s 42ms/step - loss: 0.0672 - accuracy: 0.9426 - val_loss: 0.0723 - val_accuracy: 0.9374\n",
      "Epoch 3/3\n",
      "576/576 [==============================] - 24s 42ms/step - loss: 0.0555 - accuracy: 0.9519 - val_loss: 0.0595 - val_accuracy: 0.9485\n",
      "[still updating...] Accuracy: 0.9518587095855291, Precision: 0.7446822750496102, Recall: 0.530571870576986, FScore: 0.5972842935397518\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_9 (Masking)             (None, 50)           0           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 50, 50)       72900       masking_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 50, 14)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 50, 50)       0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "masking_10 (Masking)            (None, 50, 14)       0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 50, 64)       0           dropout_5[0][0]                  \n",
      "                                                                 masking_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 50, 200)      33000       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 50, 3)        603         bidirectional_5[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 106,503\n",
      "Trainable params: 106,503\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shikhar/anaconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 144 samples\n",
      "Epoch 1/3\n",
      "576/576 [==============================] - 24s 42ms/step - loss: 0.1032 - accuracy: 0.9293 - val_loss: 0.0574 - val_accuracy: 0.9558\n",
      "Epoch 2/3\n",
      "576/576 [==============================] - 22s 39ms/step - loss: 0.0660 - accuracy: 0.9438 - val_loss: 0.0433 - val_accuracy: 0.9586\n",
      "Epoch 3/3\n",
      "576/576 [==============================] - 24s 41ms/step - loss: 0.0555 - accuracy: 0.9522 - val_loss: 0.0433 - val_accuracy: 0.9589\n",
      "[still updating...] Accuracy: 0.950858123569794, Precision: 0.7194213949530299, Recall: 0.5185923851796569, FScore: 0.5811032772504607\n",
      "Accuracy: 0.950858123569794, Precision: 0.7194213949530299, Recall: 0.5185923851796569, FScore: 0.5811032772504607\n"
     ]
    }
   ],
   "source": [
    "# Build the test and training sets of sentences.\n",
    "kf = KFold(n_splits = 5, shuffle = False)\n",
    "parsed_sentences = np.asarray(parsed_sentences)\n",
    "scores = []\n",
    "scores1 = []\n",
    "y_pred_idx = []\n",
    "y_pred_idx1 = []\n",
    "y_test_idx = []\n",
    "y_test_idx1 = []\n",
    "\n",
    "preds = []\n",
    "\n",
    "for train_index, test_index in kf.split(parsed_sentences):\n",
    "    train_data = parsed_sentences[train_index]\n",
    "    test_data = parsed_sentences[test_index]\n",
    "    X_train = [np.asarray([a[0][0] for a in train_data]), np.asarray([a[0][1] for a in train_data])]\n",
    "    Y_train = np.asarray([a[1] for a in train_data])\n",
    "    X_test = [np.asarray([a[0][0] for a in test_data]), np.asarray([a[0][1] for a in test_data])]\n",
    "    Y_test = np.asarray([a[1] for a in test_data])\n",
    "    model = create_model()\n",
    "    model.compile(optimizer = 'rmsprop',\n",
    "                  loss = 'categorical_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    model.fit(X_train, Y_train, epochs = 3, validation_split = 0.2, batch_size = 4)\n",
    "\n",
    "    y_pred_padded = np.argmax(model.predict(X_test), axis = -1)\n",
    "    y_true_padded = np.argmax(Y_test, axis = -1)\n",
    "    \n",
    "    for i in range(X_test[0].shape[0]):\n",
    "        for j in range(X_test[0].shape[1]):\n",
    "            if X_test[0][i][j] == word2idx['PAD']:\n",
    "                continue\n",
    "            else:\n",
    "                pred = y_pred_padded[i][j]\n",
    "                true = y_true_padded[i][j]\n",
    "                y_pred_idx.append(pred)\n",
    "                y_test_idx.append(true)\n",
    "                scores.append(pred == true)\n",
    "\n",
    "    prec_, rec_, fscore_, _ = precision_recall_fscore_support(y_test_idx, y_pred_idx, average = 'macro')\n",
    "    print('[still updating...] Accuracy: {}, Precision: {}, Recall: {}, FScore: {}'.format(np.asarray(scores).mean(), prec_, rec_, fscore_))\n",
    "    \n",
    "prec, rec, fscore, _ = precision_recall_fscore_support(y_test_idx, y_pred_idx, average = 'macro')\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, FScore: {}'.format(np.asarray(scores).mean(), prec, rec, fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data = [np.asarray([a[0] for a in parsed_test_sentences]), np.asarray([a[1] for a in parsed_test_sentences])]\n",
    "predictions_full = model.predict(X_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list = []\n",
    "for i, s in enumerate(test_sentences):\n",
    "    output = []\n",
    "    for j, w in enumerate(s[0]):\n",
    "        output.append(np.argmax(predictions_full[i][j]))\n",
    "    predictions_list.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NER-TestSet-RNN-Predictions.txt', 'w', encoding = 'utf-8') as f:\n",
    "    for words, predictions in zip(test_sentences, predictions_list):\n",
    "        assert(len(words[0]) == len(predictions))\n",
    "        for word, prediction in zip(words[0], predictions):\n",
    "            f.writelines(word + '\\t' + idx2labels[prediction] + '\\n')\n",
    "        f.writelines('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
