{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries.\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "import operator\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "random.seed(11)\n",
    "np.random.seed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER-Dataset-10Types-Train.txt\r\n",
      "NER-Dataset--TestSet.txt\r\n",
      "NER-Dataset-Train.txt\r\n",
      "Q1 - NER Prediction 10 Types.ipynb\r\n",
      "Q1 - NER prediction-10 Types (no test output).ipynb\r\n",
      "Q1 - NER prediction-10 Types (with test output).ipynb\r\n",
      "Q1 - NER prediction.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NER-Dataset-10Types-Train.txt', 'r') as f:\n",
    "    ner_dataset = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "words = []\n",
    "tags = []\n",
    "for line in ner_dataset:\n",
    "    line = line.strip()\n",
    "    if line == '':\n",
    "        sentences.append((words, tags))\n",
    "        words = []\n",
    "        tags = []\n",
    "    else:\n",
    "        word, tag = line.split('\\t')\n",
    "        words.append(word)\n",
    "        tags.append(tag)\n",
    "\n",
    "if len(words) > 0:\n",
    "    sentences.append((words, tags))\n",
    "    words = []\n",
    "    tags= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_counts = Counter(sum([a[0] for a in sentences], [])).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1456"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_keep = set([word for word, count in vocab_counts if count > 1])\n",
    "len(words_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_sentences = [([w if w in words_to_keep else 'UNK' for w in words], tags) for words, tags in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = set(sum([s[0] for s in sentences], []))\n",
    "# vocab2id = dict()\n",
    "# id2vocab = dict()\n",
    "# for word in vocab:\n",
    "#     vocab2id[word] = len(vocab2id)\n",
    "#     id2vocab[vocab2id[word]] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(X_train, Y_train):\n",
    "    '''\n",
    "    Function for building the vocabulary from the training set of\n",
    "    words and tags.\n",
    "    '''\n",
    "    vocabulary2id = dict()    \n",
    "    tag2id = dict()\n",
    "    vocabulary2id['UNK'] = 0\n",
    "\n",
    "    for sent in X_train:\n",
    "        for word in sent:\n",
    "            if word not in vocabulary2id.keys():\n",
    "                vocabulary2id[word] = len(vocabulary2id)\n",
    "\n",
    "    for sent in Y_train:\n",
    "        for tag in sent:\n",
    "            if tag not in tag2id.keys():\n",
    "                tag2id[tag] = len(tag2id)\n",
    "\n",
    "    return vocabulary2id, tag2id\n",
    "\n",
    "def get_word_tag_counts(X_train, Y_train, vocabulary2id, tag2id):\n",
    "    '''\n",
    "    Function for calculating the counts pertaining to the\n",
    "    individual word tags.\n",
    "    '''\n",
    "    wordcount = defaultdict(int)\n",
    "    tagcount = defaultdict(int)\n",
    "    tagpaircount = defaultdict(int)\n",
    "    tagtriplecount = defaultdict(int)\n",
    "    \n",
    "    for sent in X_train:\n",
    "        for word in sent:\n",
    "            wordcount[word] += 1\n",
    "    \n",
    "    for sent in Y_train:\n",
    "        for tag in sent:\n",
    "            tagcount[tag] += 1\n",
    "    \n",
    "    for sent in Y_train:\n",
    "        for i in range(len(sent) - 1):\n",
    "            tagpaircount[sent[i], sent[i + 1]] += 1\n",
    "\n",
    "    for sent in Y_train:\n",
    "        for i in range(len(sent) - 2):\n",
    "            tagtriplecount[sent[i], sent[i + 1], sent[i + 2]] += 1\n",
    "    \n",
    "    return wordcount, tagcount, tagpaircount, tagtriplecount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token to map all out-of-vocabulary words (OOVs).\n",
    "UNK = \"UNK\"\n",
    "# Index for UNK\n",
    "UNKid = 0\n",
    "epsilon = 1e-100\n",
    "array, ones, zeros, multiply, unravel_index = np.array, np.ones, np.zeros, np.multiply, np.unravel_index\n",
    "\n",
    "class HMM:\n",
    "    def __init__(self, state_list, observation_list, transition_proba = None,\n",
    "                 observation_proba = None, initial_state_proba = None, \n",
    "                 smoothing_obs = 0.01, transition_proba1 = None, prob_abs = 0.00001):\n",
    "        '''\n",
    "        Builds a Hidden Markov Model.\n",
    "        * state_list is the list of state symbols [q_0...q_(N-1)]\n",
    "        * observation_list is the list of observation symbols [v_0...v_(M-1)]\n",
    "        * transition_proba is the transition probability matrix\n",
    "            [a_ij] a_ij,a_ik = Pr(Y_(t+1)=q_i|Y_t=q_j,Y_(t-1)=q_k)\n",
    "        * observation_proba is the observation probablility matrix\n",
    "            [b_ki] b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "        * initial_state_proba is the initial state distribution\n",
    "            [pi_i] pi_i = Pr(Y_0=q_i)\n",
    "        '''\n",
    "        # Number of states.\n",
    "        self.N = len(state_list)\n",
    "        # Number of possible emissions.\n",
    "        self.M = len(observation_list)\n",
    "        self.prob_abs = prob_abs\n",
    "        self.omega_Y = state_list\n",
    "        self.omega_X = observation_list\n",
    "\n",
    "        if transition_proba1 is None:\n",
    "            self.transition_proba1 = zeros( (self.N, self.N), float) \n",
    "        else:\n",
    "            self.transition_proba1 = transition_proba1\n",
    "\n",
    "        if transition_proba is None:\n",
    "            self.transition_proba = zeros( (self.N, self.N, self.N), float) \n",
    "        else:\n",
    "            self.transition_proba=transition_proba\n",
    "\n",
    "        if observation_proba is None:\n",
    "            self.observation_proba = zeros( (self.M, self.N), float) \n",
    "        else:\n",
    "            self.observation_proba = observation_proba\n",
    "\n",
    "        if initial_state_proba is None:\n",
    "            self.initial_state_proba = zeros( (self.N,), float ) \n",
    "        else:\n",
    "            self.initial_state_proba = initial_state_proba\n",
    "\n",
    "        # Build indexes, i.e., the mapping between token and int.\n",
    "        self.make_indexes()\n",
    "        self.smoothing_obs = smoothing_obs \n",
    "        \n",
    "    def make_indexes(self):\n",
    "        '''\n",
    "        Function for creating the reverse table that maps\n",
    "        states/observations names to their index in the probabilities\n",
    "        array.\n",
    "        '''\n",
    "        self.Y_index = {}\n",
    "\n",
    "        for i in range(self.N):\n",
    "            self.Y_index[self.omega_Y[i]] = i\n",
    "            \n",
    "        self.X_index = {}\n",
    "            \n",
    "        for i in range(self.M):\n",
    "            self.X_index[self.omega_X[i]] = i\n",
    "        \n",
    "    def get_observationIndices(self, observations):\n",
    "        '''\n",
    "        Function for returning observation indices,\n",
    "        and dealing with OOVs.\n",
    "        '''\n",
    "        indices = zeros( len(observations), int )\n",
    "        k = 0\n",
    "\n",
    "        for o in observations:\n",
    "            if o in self.X_index:\n",
    "                indices[k] = self.X_index[o]\n",
    "            else:\n",
    "                indices[k] = UNKid\n",
    "\n",
    "            k += 1\n",
    "\n",
    "        return indices\n",
    "\n",
    "    def data2indices(self, sent): \n",
    "        '''\n",
    "        Function for extracting the words and tags and returning a\n",
    "        list of indices for each.\n",
    "        '''\n",
    "        wordids = list()\n",
    "        tagids  = list()\n",
    "\n",
    "        for couple in sent:\n",
    "            wrd = couple[0]\n",
    "            tag = couple[1]\n",
    "\n",
    "            if wrd in self.X_index:\n",
    "                wordids.append(self.X_index[wrd])\n",
    "            else:\n",
    "                wordids.append(UNKid)\n",
    "\n",
    "            tagids.append(self.Y_index[tag])\n",
    "\n",
    "        return wordids, tagids\n",
    "        \n",
    "    def observation_estimation(self, pair_counts):\n",
    "        '''\n",
    "        Function for building the observation distribution where\n",
    "        observation_proba is the observation probablility matrix.\n",
    "        '''\n",
    "        # Fill with counts.\n",
    "        for pair in pair_counts:\n",
    "            wrd = pair[0]\n",
    "            tag = pair[1]\n",
    "            cpt = pair_counts[pair]\n",
    "            # For UNK.\n",
    "            k = 0\n",
    "\n",
    "            if wrd in self.X_index: \n",
    "                k = self.X_index[wrd]\n",
    "\n",
    "            i = self.Y_index[tag]\n",
    "            self.observation_proba[k, i] = cpt\n",
    "\n",
    "        # Normalize.\n",
    "        self.observation_proba = self.observation_proba + self.smoothing_obs\n",
    "        self.observation_proba = self.observation_proba / self.observation_proba.sum(axis = 0).reshape(1, self.N)\n",
    "\n",
    "    def transition_estimation(self, trans_counts):\n",
    "        '''\n",
    "        Function for building the transition distribution where \n",
    "        transition_proba is the transition matrix with:\n",
    "        [a_ij] a[i, j] = Pr(Y_(t+1) = q_i | Y_t = q_j, Y_(t-1) = q_k)\n",
    "        '''\n",
    "        # Fill with counts.\n",
    "        for triple in trans_counts:\n",
    "            i = self.Y_index[triple[2]]\n",
    "            j = self.Y_index[triple[1]]\n",
    "            k = self.Y_index[triple[0]]\n",
    "            self.transition_proba[k, j, i] = trans_counts[triple]\n",
    "\n",
    "        # Normalize.\n",
    "        self.transition_proba = self.transition_proba / self.transition_proba.sum(axis = 0).reshape(self.N, self.N)\n",
    "\n",
    "    def transition_estimation1(self, trans_counts):\n",
    "        '''\n",
    "        Function for building the transition distribution where \n",
    "        transition_proba is the transition matrix with: \n",
    "        [a_ij] a[i,j] = Pr(Y_(t+1)=q_i|Y_t=q_j)\n",
    "        '''\n",
    "        # Fill with counts.\n",
    "        for pair in trans_counts:\n",
    "            i = self.Y_index[pair[1]]\n",
    "            j = self.Y_index[pair[0]]\n",
    "            self.transition_proba1[j, i] = trans_counts[pair]\n",
    "\n",
    "        # Normalize.\n",
    "        self.transition_proba1 = self.transition_proba1 / self.transition_proba1.sum(axis = 0).reshape(1, self.N)\n",
    "        \n",
    "    def init_estimation(self, init_counts):\n",
    "        '''\n",
    "        Function for building the initial distribution.\n",
    "        '''\n",
    "        # Fill with counts.\n",
    "        for tag in init_counts:\n",
    "            i = self.Y_index[tag]\n",
    "            self.initial_state_proba[i] = init_counts[tag]\n",
    "\n",
    "        # Normalize.\n",
    "        self.initial_state_proba = self.initial_state_proba / sum(self.initial_state_proba)\n",
    "\n",
    "    def supervised_training(self, pair_counts, trans_counts, init_counts, trans_counts1):\n",
    "        '''\n",
    "        Function for training the HMM's parameters.\n",
    "        '''\n",
    "        self.observation_estimation(pair_counts)\n",
    "        self.transition_estimation(trans_counts)\n",
    "        self.transition_estimation1(trans_counts1)\n",
    "        self.init_estimation(init_counts)\n",
    "        \n",
    "    def viterbi(self, observations):\n",
    "        if len(observations) < 2:\n",
    "            return [np.argmax(hmm.observation_proba[hmm.X_index[word]]) for z in observations]\n",
    "\n",
    "        nSamples = len(observations)\n",
    "        # Number of states.\n",
    "        nStates = self.transition_proba.shape[0]\n",
    "        # Scale factors (necessary to prevent underflow).\n",
    "        c = np.zeros(nSamples)\n",
    "        # Initialise viterbi table.\n",
    "        viterbi = np.zeros((nStates, nStates, nSamples))\n",
    "        # Initialise viterbi table.\n",
    "        viterbi1 = np.zeros((nStates, nSamples))\n",
    "        # Initialise the best path table.\n",
    "        psi = np.zeros((nStates, nStates, nSamples))\n",
    "        best_path = np.zeros(nSamples)\n",
    "        idx0 = self.X_index[observations[0]]\n",
    "        idx1 = self.X_index[observations[1]]\n",
    "        viterbi1[:, 0] = self.initial_state_proba.T * self.observation_proba[idx0, :].T\n",
    "\n",
    "        # Loop through the states.\n",
    "        for s in range (0, nStates):\n",
    "            for v in range (0, nStates):\n",
    "                viterbi[s, v, 1] = viterbi1[s, 0] * self.transition_proba1[s, v] * self.observation_proba[idx1, v]\n",
    "\n",
    "        psi[0] = 0;\n",
    "\n",
    "        # Loop through time-stamps.\n",
    "        for t in range(2, nSamples):\n",
    "            idx = self.X_index[observations[t]]\n",
    "            # Loop through the states.\n",
    "            for s in range (0, nStates):\n",
    "                for v in range (0, nStates):\n",
    "                    self.transition_proba[np.isnan(self.transition_proba)] = self.prob_abs\n",
    "                    trans_p = viterbi[:, s, t-1] * self.transition_proba[:, s, v]\n",
    "\n",
    "                    if (math.isnan(trans_p[0])):\n",
    "                        trans_p[0] = 0\n",
    "\n",
    "                    psi[s, v, t], viterbi[s, v, t] = max(enumerate(trans_p), key = operator.itemgetter(1))\n",
    "                    viterbi[s, v, t] = viterbi[s, v, t] * self.observation_proba[idx, v]\n",
    "\n",
    "        cabbar = viterbi[:, :, nSamples - 1]\n",
    "        best_path[nSamples - 1] = unravel_index(cabbar.argmax(), cabbar.shape)[1]\n",
    "        best_path[nSamples - 2] = unravel_index(cabbar.argmax(), cabbar.shape)[0]\n",
    "\n",
    "        # Return the best path, number of samples and psi.\n",
    "        for t in range(nSamples - 3, -1, -1):\n",
    "            best_path[t] = psi[int(round(best_path[t + 1])), int(round(best_path[t + 2])), t + 2]\n",
    "\n",
    "        return best_path\n",
    "        \n",
    "    def fwd_bkw(self, observations):\n",
    "        observations = x\n",
    "        self = hmm\n",
    "        nStates = self.transition_proba.shape[0]\n",
    "        start_prob = self.initial_state_proba\n",
    "        trans_prob = self.transition_proba1.transpose()\n",
    "        emm_prob = self.observation_proba.transpose()\n",
    "\n",
    "        # Forward part of the algorithm.\n",
    "        fwd = []\n",
    "        f_prev = {}\n",
    "\n",
    "        for i, observation_i in enumerate(observations):\n",
    "            f_curr = {}\n",
    "            for st in range(nStates):\n",
    "                if i == 0:\n",
    "                    # Base case for the forward part.\n",
    "                    prev_f_sum = start_prob[st]\n",
    "                else:\n",
    "                    prev_f_sum = sum(f_prev[k] * trans_prob[k][st] for k in range(nStates))\n",
    "\n",
    "                f_curr[st] = emm_prob[st][self.X_index[observation_i]] * prev_f_sum\n",
    "\n",
    "            fwd.append(f_curr)\n",
    "            f_prev = f_curr\n",
    "\n",
    "        p_fwd = sum(f_curr[k] for k in range(nStates))\n",
    "\n",
    "        # Backward part of the algorithm.\n",
    "        bkw = []\n",
    "        b_prev = {}\n",
    "\n",
    "        for i, observation_i_plus in enumerate(reversed(observations[1:] + [None,])):\n",
    "            b_curr = {}\n",
    "            for st in range(nStates):\n",
    "                if i == 0:\n",
    "                    # Base case for backward part.\n",
    "                    b_curr[st] = 1.0\n",
    "                else:\n",
    "                    b_curr[st] = sum(trans_prob[st][l] * emm_prob[l][self.X_index[observation_i_plus]]\n",
    "                                     * b_prev[l] for l in range(nStates))\n",
    "\n",
    "            bkw.insert(0,b_curr)\n",
    "            b_prev = b_curr\n",
    "\n",
    "        p_bkw = sum(start_prob[l] * emm_prob[l][self.X_index[observations[0]]] * b_curr[l] for l in range(nStates))\n",
    "\n",
    "        # Merging the two parts.\n",
    "        posterior = []\n",
    "\n",
    "        for i in range(len(observations)):\n",
    "            posterior.append({st: fwd[i][st] * bkw[i][st] / p_fwd for st in range(nStates)})\n",
    "\n",
    "        assert abs(p_fwd - p_bkw) < 1e-6\n",
    "        return fwd, bkw, posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_counts(X, Y):\n",
    "    ''' \n",
    "    Function for building the different count tables to train a HMM.\n",
    "    Each count table is a dictionary. \n",
    "    '''\n",
    "    c_words = dict()\n",
    "    c_tags = dict()\n",
    "    c_pairs= dict()\n",
    "    c_transitions = dict()\n",
    "    c_inits = dict()\n",
    "    c_transitions1 = dict()\n",
    "    \n",
    "    for sent in zip(X, Y):\n",
    "        sent = list(zip(*sent))\n",
    "        for i in range(len(sent)):\n",
    "            couple = sent[i]\n",
    "            wrd = couple[0]\n",
    "            tag = couple[1]\n",
    "\n",
    "            # Word counts.\n",
    "            if wrd in c_words:\n",
    "                c_words[wrd] = c_words[wrd] + 1\n",
    "            else:\n",
    "                c_words[wrd] = 1\n",
    "\n",
    "            # Tag counts.\n",
    "            if tag in c_tags:\n",
    "                c_tags[tag] = c_tags[tag] + 1\n",
    "            else:\n",
    "                c_tags[tag] = 1\n",
    "\n",
    "            # Observation counts.\n",
    "            if couple in c_pairs:\n",
    "                c_pairs[couple] = c_pairs[couple] + 1\n",
    "            else:\n",
    "                c_pairs[couple] = 1\n",
    "\n",
    "            if i >= 1:\n",
    "                trans1 = (sent[i - 1][1], tag)\n",
    "                if trans1 in c_transitions1:\n",
    "                    c_transitions1[trans1] = c_transitions1[trans1] + 1\n",
    "                else:\n",
    "                    c_transitions1[trans1] = 1\n",
    "\n",
    "            if i > 1:\n",
    "                trans = (sent[i - 2][1], sent[i - 1][1], tag)\n",
    "                if trans in c_transitions:\n",
    "                    c_transitions[trans] = c_transitions[trans] + 1\n",
    "                else:\n",
    "                    c_transitions[trans] = 1\n",
    "            else:\n",
    "                if tag in c_inits:\n",
    "                    c_inits[tag] = c_inits[tag] + 1\n",
    "                else:\n",
    "                    c_inits[tag] = 1\n",
    "    \n",
    "    return c_words,c_tags,c_pairs, c_transitions, c_inits, c_transitions1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/btech/cse/2016/mukuntha.cs16/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:147: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy and Scores:\n",
      "Viterbi Accuracy: 0.9012013729977116, Precision: 0.17299751704223215, Recall: 0.1306308711792958, FScore: 0.1416866524575724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/btech/cse/2016/mukuntha.cs16/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Build the test and training sets of sentences.\n",
    "kf = KFold(n_splits = 5, shuffle = False)\n",
    "parsed_sentences = np.asarray(parsed_sentences)\n",
    "scores = []\n",
    "scores1 = []\n",
    "y_pred_idx = []\n",
    "y_pred_idx1 = []\n",
    "y_test_idx = []\n",
    "y_test_idx1 = []\n",
    "\n",
    "preds = []\n",
    "\n",
    "for train_index, test_index in kf.split(parsed_sentences):\n",
    "    train_data = parsed_sentences[train_index]\n",
    "    test_data = parsed_sentences[test_index]\n",
    "    X_train = [a[0] for a in train_data]\n",
    "    Y_train = [a[1] for a in train_data]\n",
    "    X_test = [a[0] for a in test_data]\n",
    "    Y_test = [a[1] for a in test_data]\n",
    "    \n",
    "    # Build the vocabulary and word counts.\n",
    "    vocabulary2id, tag2id = get_vocab(X_train, Y_train)\n",
    "    wordcount, tagcount, tagpaircount, tagtriplecount = get_word_tag_counts(X_train, Y_train, vocabulary2id, tag2id)\n",
    "    cwords, ctags, cpairs, ctrans, cinits, ctrans1 = make_counts(X_train, Y_train)\n",
    "    \n",
    "    state_list = list(ctags.keys())\n",
    "    observation_list = [a[0] for a in sorted(vocabulary2id.items(), key = lambda x: x[1])]\n",
    "    hmm = HMM(state_list = state_list, observation_list = observation_list,\n",
    "              transition_proba = None, observation_proba = None, initial_state_proba = None,\n",
    "              smoothing_obs = 0.4, prob_abs = 0)\n",
    "    hmm.supervised_training(cpairs, ctrans, cinits, ctrans1)\n",
    "\n",
    "    for x, y_true in zip(X_test, Y_test):\n",
    "        for i in range(len(x)):\n",
    "            if x[i] not in vocabulary2id.keys():\n",
    "                x[i] = 'UNK'\n",
    "\n",
    "        pred_idx = hmm.viterbi(x)\n",
    "        y_pred = np.asarray([state_list[int(round(i))] for i in pred_idx])\n",
    "        y_true = np.asarray(y_true)\n",
    "        y_pred_idx += np.asarray([tag2id[lab] for lab in y_pred], dtype = np.int32).tolist()\n",
    "        y_test_idx += np.asarray([tag2id[lab] for lab in y_true], dtype = np.int32).tolist()\n",
    "        scores += (y_pred == y_true).tolist()\n",
    "\n",
    "    x, y_true = X_train[0], Y_train[0]\n",
    "\n",
    "    for x, y_true in zip(X_test, Y_test):\n",
    "        for i in range(len(x)):\n",
    "            if x[i] not in vocabulary2id.keys():\n",
    "                x[i] = 'UNK'\n",
    "\n",
    "        pred_probs = hmm.fwd_bkw(x)\n",
    "        pred_idx = [max(probs.items(), key=lambda x: x[1])[0] for probs in pred_probs[2]]\n",
    "        y_pred = np.asarray([state_list[int(round(i))] for i in pred_idx])\n",
    "        y_true = np.asarray(y_true)\n",
    "        y_pred_idx1 += np.asarray([tag2id[lab] for lab in y_pred], dtype = np.int32).tolist()\n",
    "        y_test_idx1 += np.asarray([tag2id[lab] for lab in y_true], dtype = np.int32).tolist()\n",
    "        scores1 += (y_pred == y_true).tolist()\n",
    "\n",
    "prec, rec, fscore, _ = precision_recall_fscore_support(y_test_idx, y_pred_idx, average = 'macro')\n",
    "prec1, rec1, fscore1, _ = precision_recall_fscore_support(y_test_idx1, y_pred_idx1, average = 'macro')\n",
    "\n",
    "print('Overall Accuracy and Scores:')\n",
    "# print('Only Forward-Backward Accuracy: {}, Precision: {}, Recall: {}, FScore: {}'.format(np.asarray(scores1).mean(), prec1, rec1, fscore1))\n",
    "print('Viterbi Accuracy: {}, Precision: {}, Recall: {}, FScore: {}'.format(np.asarray(scores).mean(), prec, rec, fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NER-Dataset--TestSet.txt', 'r') as f:\n",
    "    test_dataset = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = []\n",
    "words = []\n",
    "for line in test_dataset:\n",
    "    line = line.strip()\n",
    "    if line == '':\n",
    "        test_sentences.append((words,))\n",
    "        words = []\n",
    "    else:\n",
    "        word = line\n",
    "        words.append(word)\n",
    "\n",
    "if len(words) > 0:\n",
    "    test_sentences.append((words,))\n",
    "    words = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['@SammieLynnsMom',\n",
       "   '@tg1.781',\n",
       "   'they',\n",
       "   'will',\n",
       "   'be',\n",
       "   'all',\n",
       "   'done',\n",
       "   'by',\n",
       "   'Sunday',\n",
       "   'trust',\n",
       "   'me',\n",
       "   '*wink*'],),\n",
       " (['Made',\n",
       "   'it',\n",
       "   'back',\n",
       "   'home',\n",
       "   'to',\n",
       "   'GA',\n",
       "   '.',\n",
       "   'It',\n",
       "   'sucks',\n",
       "   'not',\n",
       "   'to',\n",
       "   'be',\n",
       "   'at',\n",
       "   'Disney',\n",
       "   'world',\n",
       "   ',',\n",
       "   'but',\n",
       "   'its',\n",
       "   'good',\n",
       "   'to',\n",
       "   'be',\n",
       "   'home',\n",
       "   '.',\n",
       "   'Time',\n",
       "   'to',\n",
       "   'start',\n",
       "   'planning',\n",
       "   'the',\n",
       "   'next',\n",
       "   'Disney',\n",
       "   'World',\n",
       "   'trip',\n",
       "   '.'],),\n",
       " ([\"'\",\n",
       "   'Breaking',\n",
       "   'Dawn',\n",
       "   \"'\",\n",
       "   'Returns',\n",
       "   'to',\n",
       "   'Vancouver',\n",
       "   'on',\n",
       "   'January',\n",
       "   '11th',\n",
       "   'http://bit.ly/dbDMs8'],),\n",
       " (['@ls_n',\n",
       "   'perhaps',\n",
       "   ',',\n",
       "   'but',\n",
       "   'folks',\n",
       "   'may',\n",
       "   'find',\n",
       "   'something',\n",
       "   'in',\n",
       "   'the',\n",
       "   'gallery',\n",
       "   'that',\n",
       "   'is',\n",
       "   'helpful',\n",
       "   'in',\n",
       "   'their',\n",
       "   'day-to-day',\n",
       "   'work',\n",
       "   'as',\n",
       "   'well',\n",
       "   '.',\n",
       "   'Even',\n",
       "   'just',\n",
       "   'to',\n",
       "   'use',\n",
       "   'it',\n",
       "   '.'],),\n",
       " (['@Carr.t', 'aye', 'been', 'tonight', '-', 'excellent'],),\n",
       " (['RT',\n",
       "   '@LilTwist',\n",
       "   ':',\n",
       "   'RT',\n",
       "   'this',\n",
       "   'if',\n",
       "   'you',\n",
       "   'want',\n",
       "   'me',\n",
       "   'to',\n",
       "   'go',\n",
       "   'back',\n",
       "   'live',\n",
       "   'on',\n",
       "   'Ustream',\n",
       "   'later',\n",
       "   'tonight'],),\n",
       " (['@Hollly_', '16', 'b', '17', 'in', 'feb'],),\n",
       " (['RT',\n",
       "   '@obsidianchao',\n",
       "   ':',\n",
       "   'OF',\n",
       "   'FUCKING',\n",
       "   'COURSE',\n",
       "   '.',\n",
       "   'I',\n",
       "   'GET',\n",
       "   'HOME',\n",
       "   'AND',\n",
       "   'MY',\n",
       "   'BROTHER',\n",
       "   'IS',\n",
       "   'ON',\n",
       "   'THE',\n",
       "   'MOTHER',\n",
       "   'FUCKING',\n",
       "   'XBOX',\n",
       "   '.',\n",
       "   'Worst',\n",
       "   'fucking',\n",
       "   'day',\n",
       "   'ever',\n",
       "   '.'],),\n",
       " (['I',\n",
       "   \"haven't\",\n",
       "   'driven',\n",
       "   'to',\n",
       "   'bc',\n",
       "   'in',\n",
       "   'years',\n",
       "   ',',\n",
       "   'and',\n",
       "   'I',\n",
       "   'am',\n",
       "   'just',\n",
       "   'stunned',\n",
       "   'by',\n",
       "   'how',\n",
       "   'beautiful',\n",
       "   'the',\n",
       "   'drive',\n",
       "   'is',\n",
       "   '.'],),\n",
       " (['@daraobriain',\n",
       "   'hmmm',\n",
       "   '.',\n",
       "   'Cant',\n",
       "   'wait',\n",
       "   '.',\n",
       "   'Comin',\n",
       "   'on',\n",
       "   'Thursday',\n",
       "   '.',\n",
       "   'First',\n",
       "   'time',\n",
       "   'to',\n",
       "   'the',\n",
       "   'Apollo',\n",
       "   '.'],),\n",
       " (['RT',\n",
       "   '@Hatshepsutely',\n",
       "   ':',\n",
       "   '@adamlambert',\n",
       "   'please',\n",
       "   ',',\n",
       "   'oh',\n",
       "   'please',\n",
       "   'wear',\n",
       "   'the',\n",
       "   'infamous',\n",
       "   'beach',\n",
       "   'hat',\n",
       "   'tonight',\n",
       "   'during',\n",
       "   'your',\n",
       "   'encore',\n",
       "   '(',\n",
       "   'in',\n",
       "   'lieu',\n",
       "   'of',\n",
       "   'a',\n",
       "   'rasta',\n",
       "   'wig)',\n",
       "   '.',\n",
       "   '&lt;',\n",
       "   '3333'],),\n",
       " (['When',\n",
       "   'I',\n",
       "   'said',\n",
       "   'good',\n",
       "   'night',\n",
       "   ',',\n",
       "   'I',\n",
       "   'ended',\n",
       "   'up',\n",
       "   'watching',\n",
       "   'movies',\n",
       "   'on',\n",
       "   'my',\n",
       "   'laptop',\n",
       "   '.',\n",
       "   'Yep',\n",
       "   '.',\n",
       "   'No',\n",
       "   'sleep',\n",
       "   'again',\n",
       "   '.',\n",
       "   'Time',\n",
       "   'to',\n",
       "   'take',\n",
       "   'a',\n",
       "   'bath',\n",
       "   'now',\n",
       "   '!',\n",
       "   'It',\n",
       "   \"'s\",\n",
       "   'gonna',\n",
       "   'be',\n",
       "   'a',\n",
       "   'looong',\n",
       "   'day',\n",
       "   '.'],),\n",
       " ([\"'On\",\n",
       "   'set',\n",
       "   'filming',\n",
       "   'Chris',\n",
       "   '\"',\n",
       "   'Rainy',\n",
       "   'Days',\n",
       "   '\"',\n",
       "   'video',\n",
       "   '.',\n",
       "   'This',\n",
       "   'is',\n",
       "   'the',\n",
       "   'final',\n",
       "   'scene',\n",
       "   'then',\n",
       "   'we',\n",
       "   'can',\n",
       "   'finally',\n",
       "   'edit',\n",
       "   'it',\n",
       "   \".'\"],),\n",
       " (['@EimearJedward', 'auditioning', 'on', 'sunday', ':)', 'xo'],),\n",
       " (['@SheSo_Major',\n",
       "   'they',\n",
       "   'giving',\n",
       "   'the',\n",
       "   'nigga',\n",
       "   'a',\n",
       "   'realllllll',\n",
       "   'hard',\n",
       "   'time',\n",
       "   'about',\n",
       "   'it',\n",
       "   'lmfaoo',\n",
       "   'as',\n",
       "   'they',\n",
       "   'should',\n",
       "   'and',\n",
       "   'now',\n",
       "   'his',\n",
       "   'feelings',\n",
       "   'are',\n",
       "   'hurt',\n",
       "   '...',\n",
       "   'i',\n",
       "   'like',\n",
       "   'him',\n",
       "   'tho',\n",
       "   'LMAO'],),\n",
       " (['The',\n",
       "   'Wolves',\n",
       "   'to',\n",
       "   'host',\n",
       "   'the',\n",
       "   'Lions',\n",
       "   'for',\n",
       "   'game',\n",
       "   'time',\n",
       "   '!',\n",
       "   'Follow',\n",
       "   'us',\n",
       "   'for',\n",
       "   'scores',\n",
       "   'and',\n",
       "   'updates',\n",
       "   'throughout',\n",
       "   'the',\n",
       "   'game',\n",
       "   '!'],),\n",
       " (['Man',\n",
       "   'gets',\n",
       "   '2.',\n",
       "   'years',\n",
       "   'in',\n",
       "   'murder',\n",
       "   'plot',\n",
       "   'against',\n",
       "   'judge',\n",
       "   'http://bit.ly/agB9iL'],),\n",
       " (['...',\n",
       "   'around',\n",
       "   'all',\n",
       "   'day',\n",
       "   'isnt',\n",
       "   'good',\n",
       "   'for',\n",
       "   'you',\n",
       "   ',',\n",
       "   'least',\n",
       "   'i',\n",
       "   'get',\n",
       "   'home-relaxing',\n",
       "   'time',\n",
       "   ',',\n",
       "   'thursday',\n",
       "   'and',\n",
       "   'frees',\n",
       "   'to',\n",
       "   'keeep',\n",
       "   'me',\n",
       "   'sane',\n",
       "   ':)'],),\n",
       " (['Today',\n",
       "   'is',\n",
       "   'the',\n",
       "   'COMPLETE',\n",
       "   'OPPOSITE',\n",
       "   'of',\n",
       "   'yesterday',\n",
       "   '-',\n",
       "   '.',\n",
       "   '-'],),\n",
       " (['#SIUC',\n",
       "   'Whats',\n",
       "   'the',\n",
       "   'Plan',\n",
       "   'For',\n",
       "   'tonight',\n",
       "   '??',\n",
       "   'Whos',\n",
       "   'Goin',\n",
       "   'to',\n",
       "   'the',\n",
       "   'BLAST'],),\n",
       " (['Just',\n",
       "   'changed',\n",
       "   'my',\n",
       "   'twitter',\n",
       "   'background',\n",
       "   ',',\n",
       "   'check',\n",
       "   'it',\n",
       "   'out',\n",
       "   '!',\n",
       "   'Found',\n",
       "   'it',\n",
       "   'at',\n",
       "   'http://twitrounds.com',\n",
       "   '..',\n",
       "   'September',\n",
       "   '17',\n",
       "   ',',\n",
       "   '2.1.',\n",
       "   ',',\n",
       "   '2:52',\n",
       "   'pm'],),\n",
       " (['time',\n",
       "   'to',\n",
       "   'get',\n",
       "   'some',\n",
       "   'sleep',\n",
       "   '.',\n",
       "   'been',\n",
       "   'out',\n",
       "   'from',\n",
       "   '4pm',\n",
       "   'till',\n",
       "   'now',\n",
       "   '.'],),\n",
       " ([\"'Come\",\n",
       "   'to',\n",
       "   '\"',\n",
       "   'MRB',\n",
       "   '@',\n",
       "   '61',\n",
       "   'Roadhouse',\n",
       "   'BBQ',\n",
       "   '\"',\n",
       "   'Sunday',\n",
       "   ',',\n",
       "   'October',\n",
       "   '1.',\n",
       "   'from',\n",
       "   '12:..',\n",
       "   'pm',\n",
       "   'to',\n",
       "   '4:..',\n",
       "   'pm',\n",
       "   '.',\n",
       "   'We',\n",
       "   'are',\n",
       "   'playing',\n",
       "   'for',\n",
       "   'the',\n",
       "   '61',\n",
       "   'Roadhouse',\n",
       "   '...',\n",
       "   \"http://fb.me/G1FbJGw1'\"],),\n",
       " (['RT',\n",
       "   '@OMBieberJokes',\n",
       "   ':',\n",
       "   'Little',\n",
       "   'Girl',\n",
       "   ':',\n",
       "   'Santa',\n",
       "   'Is',\n",
       "   'So',\n",
       "   'Awesome',\n",
       "   '!',\n",
       "   'Me',\n",
       "   ':',\n",
       "   'No',\n",
       "   'Little',\n",
       "   'Girl',\n",
       "   '...',\n",
       "   'You',\n",
       "   'Only',\n",
       "   'Use',\n",
       "   'The',\n",
       "   'Word',\n",
       "   '\"',\n",
       "   'Awesome',\n",
       "   '\"',\n",
       "   'When',\n",
       "   'You',\n",
       "   'Are',\n",
       "   'Referring',\n",
       "   'To',\n",
       "   'Just',\n",
       "   '...'],),\n",
       " (['Today',\n",
       "   '=',\n",
       "   'Morton',\n",
       "   \"'s\",\n",
       "   'Salt',\n",
       "   'motto',\n",
       "   '+',\n",
       "   'Murphy',\n",
       "   \"'s\",\n",
       "   'Law'],),\n",
       " (['@FLAM3Z13.',\n",
       "   'half',\n",
       "   'the',\n",
       "   'time',\n",
       "   'in',\n",
       "   'inglewood',\n",
       "   'the',\n",
       "   'other',\n",
       "   'half',\n",
       "   'in',\n",
       "   'mv',\n",
       "   '...',\n",
       "   'where',\n",
       "   'u',\n",
       "   'been'],),\n",
       " (['RT',\n",
       "   '@shan7792',\n",
       "   ':',\n",
       "   'its',\n",
       "   'annoying',\n",
       "   'tht',\n",
       "   'ppl',\n",
       "   'ask',\n",
       "   'me',\n",
       "   'what',\n",
       "   'race',\n",
       "   'i',\n",
       "   'am',\n",
       "   'before',\n",
       "   'they',\n",
       "   'ask',\n",
       "   'me',\n",
       "   'my',\n",
       "   'name',\n",
       "   '-_-'],),\n",
       " (['My', 'boobs', 'look', 'HUGE', 'today', '!', '#NAME?'],),\n",
       " (['@tanyahowse',\n",
       "   'ok',\n",
       "   'this',\n",
       "   'may',\n",
       "   'sound',\n",
       "   'stupid',\n",
       "   'but',\n",
       "   'can',\n",
       "   'u',\n",
       "   'tell',\n",
       "   'them',\n",
       "   'jedsweemonster',\n",
       "   'says',\n",
       "   'hi',\n",
       "   \":'\",\n",
       "   ')'],),\n",
       " (['Serious',\n",
       "   'Eats',\n",
       "   ':',\n",
       "   'Leftovers',\n",
       "   ':',\n",
       "   'The',\n",
       "   'Day',\n",
       "   \"'s\",\n",
       "   'Stray',\n",
       "   'Links',\n",
       "   '#Food',\n",
       "   'http://portfo.li/t/7/rjE'],),\n",
       " (['What',\n",
       "   'is',\n",
       "   'it',\n",
       "   'with',\n",
       "   'people',\n",
       "   'and',\n",
       "   'punctuality',\n",
       "   '?',\n",
       "   'I',\n",
       "   'used',\n",
       "   'to',\n",
       "   'be',\n",
       "   'that',\n",
       "   'guy',\n",
       "   'and',\n",
       "   'honestly',\n",
       "   ',',\n",
       "   'it',\n",
       "   \"'s\",\n",
       "   'a',\n",
       "   'haze',\n",
       "   'now',\n",
       "   '.',\n",
       "   'Just',\n",
       "   'be',\n",
       "   'on',\n",
       "   'time',\n",
       "   '.',\n",
       "   'It',\n",
       "   \"'s\",\n",
       "   'not',\n",
       "   'like',\n",
       "   'its',\n",
       "   'a',\n",
       "   'surprise',\n",
       "   '.'],),\n",
       " (['he',\n",
       "   'likes',\n",
       "   'prince',\n",
       "   ',',\n",
       "   'paul',\n",
       "   'simon',\n",
       "   ',',\n",
       "   'and',\n",
       "   'o.d.b.',\n",
       "   'i',\n",
       "   'may',\n",
       "   'have',\n",
       "   'found',\n",
       "   'my',\n",
       "   'soul',\n",
       "   'mate',\n",
       "   '.'],),\n",
       " (['Come',\n",
       "   'to',\n",
       "   '\"',\n",
       "   '6th',\n",
       "   'Biannual',\n",
       "   '24',\n",
       "   'Hour',\n",
       "   'Prayer',\n",
       "   'Focus',\n",
       "   '\"',\n",
       "   'Saturday',\n",
       "   ',',\n",
       "   'November',\n",
       "   '13',\n",
       "   'from',\n",
       "   '1.:..',\n",
       "   'am',\n",
       "   'to',\n",
       "   '1:..',\n",
       "   'pm',\n",
       "   '.',\n",
       "   'Mark',\n",
       "   'the',\n",
       "   'Date',\n",
       "   '!!!',\n",
       "   'http://fb.me/JyYXPmql'],),\n",
       " (['@RaniLovezYou',\n",
       "   'Yeah',\n",
       "   ',',\n",
       "   'he',\n",
       "   \"'s\",\n",
       "   'had',\n",
       "   'a',\n",
       "   'few',\n",
       "   'shows',\n",
       "   'over',\n",
       "   'here',\n",
       "   '.',\n",
       "   'Great',\n",
       "   'chef',\n",
       "   ',',\n",
       "   'glad',\n",
       "   'I',\n",
       "   \"wasn't\",\n",
       "   'disappointed',\n",
       "   '!',\n",
       "   'His',\n",
       "   'wife',\n",
       "   'gave',\n",
       "   'birth',\n",
       "   'a',\n",
       "   'couple',\n",
       "   'of',\n",
       "   'days',\n",
       "   'ago',\n",
       "   ',',\n",
       "   'aww',\n",
       "   '.'],),\n",
       " ([\"I'm\", 'tired', 'after', 'school', 'today', '!'],),\n",
       " (['It',\n",
       "   \"'s\",\n",
       "   'supposed',\n",
       "   'to',\n",
       "   'rain',\n",
       "   'in',\n",
       "   'The',\n",
       "   'Bay',\n",
       "   'on',\n",
       "   'Sunday',\n",
       "   '...',\n",
       "   'well',\n",
       "   ',',\n",
       "   'weather.com',\n",
       "   'says',\n",
       "   '&quot;',\n",
       "   'showers',\n",
       "   '&quot;.',\n",
       "   'Whatever',\n",
       "   ',',\n",
       "   'same',\n",
       "   'thing',\n",
       "   '.'],),\n",
       " (['cant',\n",
       "   'wait',\n",
       "   'to',\n",
       "   'see',\n",
       "   'my',\n",
       "   'beautiful',\n",
       "   'neice',\n",
       "   'nx',\n",
       "   'week',\n",
       "   ',',\n",
       "   'gonna',\n",
       "   'spoil',\n",
       "   'her',\n",
       "   'rotton'],),\n",
       " (['GuitarNews',\n",
       "   ':',\n",
       "   'RockHouse',\n",
       "   ':',\n",
       "   'The',\n",
       "   'Weekend',\n",
       "   'Read',\n",
       "   'For',\n",
       "   'Sept',\n",
       "   '.',\n",
       "   '17th',\n",
       "   '2.1.',\n",
       "   '-',\n",
       "   'Guitar',\n",
       "   'Lessons',\n",
       "   ',',\n",
       "   'Articles',\n",
       "   'and',\n",
       "   'More',\n",
       "   ':',\n",
       "   'http://bit.ly/bmHnln'],),\n",
       " (['What', 'a', 'productive', 'day', '.', 'Not', '.'],),\n",
       " (['right',\n",
       "   '.',\n",
       "   'Five',\n",
       "   'Star',\n",
       "   'Day',\n",
       "   'came',\n",
       "   'out',\n",
       "   'in',\n",
       "   'may',\n",
       "   'this',\n",
       "   'year',\n",
       "   '.',\n",
       "   'why',\n",
       "   \"haven't\",\n",
       "   'ANY',\n",
       "   'trailers',\n",
       "   'come',\n",
       "   'out',\n",
       "   'here',\n",
       "   'in',\n",
       "   'Scotland',\n",
       "   'yet',\n",
       "   '?',\n",
       "   \"i've\",\n",
       "   'waited',\n",
       "   'about',\n",
       "   'a',\n",
       "   'year',\n",
       "   'for',\n",
       "   'this',\n",
       "   '!'],),\n",
       " (['@jeffpulver',\n",
       "   '@wildman94',\n",
       "   '@MusicIsGood4U',\n",
       "   'thanks',\n",
       "   'for',\n",
       "   'hitting',\n",
       "   'me',\n",
       "   'and',\n",
       "   '@shoewolf',\n",
       "   'up',\n",
       "   '.',\n",
       "   'Let',\n",
       "   'me',\n",
       "   'know',\n",
       "   'when',\n",
       "   'that',\n",
       "   'DM',\n",
       "   'with',\n",
       "   'all',\n",
       "   'the',\n",
       "   'info',\n",
       "   'has',\n",
       "   'been',\n",
       "   'sent',\n",
       "   '.'],),\n",
       " (['is',\n",
       "   'up',\n",
       "   'and',\n",
       "   'ready',\n",
       "   'for',\n",
       "   'his',\n",
       "   'last',\n",
       "   'day',\n",
       "   'on',\n",
       "   'the',\n",
       "   'punt',\n",
       "   'until',\n",
       "   'Stakes',\n",
       "   'Day',\n",
       "   '...',\n",
       "   '#sadbuttrue'],),\n",
       " (['@alealshinn',\n",
       "   '@BambingBling',\n",
       "   '~~~',\n",
       "   '@sunchips',\n",
       "   'this',\n",
       "   'is',\n",
       "   'the',\n",
       "   'second',\n",
       "   'time',\n",
       "   'they',\n",
       "   'have',\n",
       "   'messed',\n",
       "   'up',\n",
       "   '....',\n",
       "   'about',\n",
       "   'to',\n",
       "   'give',\n",
       "   'up',\n",
       "   'on',\n",
       "   'them'],),\n",
       " (['@Daniim92',\n",
       "   'tks',\n",
       "   ':D',\n",
       "   'obviously',\n",
       "   'its',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'happiest',\n",
       "   'day',\n",
       "   'in',\n",
       "   'my',\n",
       "   'life',\n",
       "   'lol'],),\n",
       " (['RT',\n",
       "   '@mrdaveyd',\n",
       "   ':',\n",
       "   'PG&lt;E',\n",
       "   'donated',\n",
       "   '35G',\n",
       "   'to',\n",
       "   'state',\n",
       "   'committe',\n",
       "   'chairman',\n",
       "   'investigating',\n",
       "   'them',\n",
       "   '1day',\n",
       "   'after',\n",
       "   'explosion',\n",
       "   'in',\n",
       "   '#sanBrunofire',\n",
       "   '-Who',\n",
       "   'is',\n",
       "   'still',\n",
       "   'defendi',\n",
       "   '...'],),\n",
       " (['@coulditbeJenG',\n",
       "   'It',\n",
       "   'was',\n",
       "   '2.',\n",
       "   'years',\n",
       "   'ago',\n",
       "   '.',\n",
       "   \"They're\",\n",
       "   'coming',\n",
       "   'after',\n",
       "   'me',\n",
       "   ',',\n",
       "   'I',\n",
       "   'just',\n",
       "   'know',\n",
       "   'it',\n",
       "   '.'],),\n",
       " (['@peasmom3',\n",
       "   'and',\n",
       "   'as',\n",
       "   'if',\n",
       "   'I',\n",
       "   'was',\n",
       "   \"n't\",\n",
       "   'obsessed',\n",
       "   'with',\n",
       "   'AFL',\n",
       "   'already',\n",
       "   ',',\n",
       "   'after',\n",
       "   'Sunday',\n",
       "   'I',\n",
       "   \"'ve\",\n",
       "   'gotten',\n",
       "   'even',\n",
       "   'more',\n",
       "   'so',\n",
       "   '!!',\n",
       "   'I',\n",
       "   \"'m\",\n",
       "   'so',\n",
       "   'obsessed',\n",
       "   'with',\n",
       "   'his',\n",
       "   'sexiness',\n",
       "   '!!!!'],),\n",
       " (['Of',\n",
       "   'course',\n",
       "   ',',\n",
       "   'I',\n",
       "   'was',\n",
       "   'stunned',\n",
       "   '.',\n",
       "   'This',\n",
       "   ',',\n",
       "   'after',\n",
       "   'all',\n",
       "   ',',\n",
       "   'was',\n",
       "   'an',\n",
       "   'inanimate',\n",
       "   'object',\n",
       "   '.'],),\n",
       " (['Halo', 'Reach', 'was', 'a', 'bit', 'crap', 'tonight'],),\n",
       " (['So',\n",
       "   'here',\n",
       "   'in',\n",
       "   'Belgium',\n",
       "   'its',\n",
       "   'now',\n",
       "   '7minutes',\n",
       "   'before',\n",
       "   'midnight',\n",
       "   ',,',\n",
       "   'Tell',\n",
       "   'me',\n",
       "   'how',\n",
       "   'late',\n",
       "   'it',\n",
       "   'is',\n",
       "   'were',\n",
       "   'you',\n",
       "   'live',\n",
       "   ':D'],),\n",
       " (['We',\n",
       "   'are',\n",
       "   'tied',\n",
       "   'as',\n",
       "   'the',\n",
       "   '5th',\n",
       "   'band',\n",
       "   'to',\n",
       "   'compete',\n",
       "   'in',\n",
       "   'South',\n",
       "   '1.7',\n",
       "   \"'s\",\n",
       "   'Battle',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Bands',\n",
       "   'next',\n",
       "   'Friday',\n",
       "   'for',\n",
       "   'the',\n",
       "   'spot',\n",
       "   'to',\n",
       "   'open',\n",
       "   'for',\n",
       "   '...',\n",
       "   'http://fb.me/IxhGnUhs'],),\n",
       " (['RT',\n",
       "   '@eljmayes',\n",
       "   ':',\n",
       "   'Ladbrokes',\n",
       "   'Labour',\n",
       "   'Leadership',\n",
       "   'Market-',\n",
       "   'http://bit.ly/cD3Rn8',\n",
       "   'Ed',\n",
       "   'Miliband',\n",
       "   \"'s\",\n",
       "   'odds',\n",
       "   'have',\n",
       "   'shortened',\n",
       "   'significantly',\n",
       "   'in',\n",
       "   'the',\n",
       "   'last',\n",
       "   'week',\n",
       "   '.'],),\n",
       " (['@Zerinaakers',\n",
       "   'talking',\n",
       "   'about',\n",
       "   'as',\n",
       "   'far',\n",
       "   'as',\n",
       "   'shooting',\n",
       "   'for',\n",
       "   'location',\n",
       "   'its',\n",
       "   'going',\n",
       "   'to',\n",
       "   'be',\n",
       "   'on',\n",
       "   '#Vh1',\n",
       "   'so',\n",
       "   'by',\n",
       "   'the',\n",
       "   'end',\n",
       "   'of',\n",
       "   'the',\n",
       "   'month',\n",
       "   'i',\n",
       "   'should',\n",
       "   'know',\n",
       "   'something'],),\n",
       " (['First',\n",
       "   'Day',\n",
       "   'of',\n",
       "   'Autumn',\n",
       "   'Networking',\n",
       "   'Mixer',\n",
       "   'at',\n",
       "   'Dick',\n",
       "   \"'s\",\n",
       "   'Carpet',\n",
       "   'next',\n",
       "   'T',\n",
       "   '...',\n",
       "   'http://conta.cc/ap95gL',\n",
       "   'via',\n",
       "   '#constantcontact'],),\n",
       " (['Why',\n",
       "   'hate',\n",
       "   'on',\n",
       "   'somebody',\n",
       "   'for',\n",
       "   'doing',\n",
       "   'well',\n",
       "   'when',\n",
       "   'you',\n",
       "   'can',\n",
       "   'simply',\n",
       "   'say',\n",
       "   'great',\n",
       "   'job',\n",
       "   'and',\n",
       "   'move',\n",
       "   'on',\n",
       "   '..',\n",
       "   '[Haters',\n",
       "   'u',\n",
       "   'can',\n",
       "   'kill',\n",
       "   'yourself',\n",
       "   ']',\n",
       "   'cuz',\n",
       "   'ur',\n",
       "   'losers'],),\n",
       " (['Wat',\n",
       "   'a',\n",
       "   'day',\n",
       "   'today',\n",
       "   '.',\n",
       "   'But',\n",
       "   'oh',\n",
       "   'well',\n",
       "   'i',\n",
       "   'love',\n",
       "   'her',\n",
       "   '*Danielle',\n",
       "   'ily*'],),\n",
       " (['@Vivianna_loves',\n",
       "   'I',\n",
       "   'just',\n",
       "   'bought',\n",
       "   'Dior',\n",
       "   'mascara',\n",
       "   'for',\n",
       "   'the',\n",
       "   'first',\n",
       "   'time',\n",
       "   '.....',\n",
       "   'its',\n",
       "   'the',\n",
       "   'only',\n",
       "   'mascara',\n",
       "   'that',\n",
       "   \"doesn't\",\n",
       "   'make',\n",
       "   'me',\n",
       "   'wanna',\n",
       "   'wear',\n",
       "   'lashes',\n",
       "   '.'],),\n",
       " (['realy',\n",
       "   'bored',\n",
       "   'smack',\n",
       "   'down',\n",
       "   'tonite',\n",
       "   'yay',\n",
       "   '!!',\n",
       "   '(',\n",
       "   ':',\n",
       "   'mood',\n",
       "   ':',\n",
       "   'bored-_-',\n",
       "   'ha',\n",
       "   '!'],),\n",
       " (['Bloggers',\n",
       "   ':',\n",
       "   'Who',\n",
       "   'are',\n",
       "   'you',\n",
       "   'writing',\n",
       "   'for',\n",
       "   '?',\n",
       "   '|',\n",
       "   'Social',\n",
       "   'Media',\n",
       "   'Today',\n",
       "   'http://bit.ly/9mY.NK'],),\n",
       " (['RT',\n",
       "   '@LilTwist',\n",
       "   ':',\n",
       "   'RT',\n",
       "   'this',\n",
       "   'if',\n",
       "   'you',\n",
       "   'want',\n",
       "   'me',\n",
       "   'to',\n",
       "   'go',\n",
       "   'back',\n",
       "   'live',\n",
       "   'on',\n",
       "   'Ustream',\n",
       "   'later',\n",
       "   'tonight'],),\n",
       " (['The',\n",
       "   'British',\n",
       "   'have',\n",
       "   'a',\n",
       "   'remarkable',\n",
       "   'talent',\n",
       "   'for',\n",
       "   'keeping',\n",
       "   'calm',\n",
       "   ',',\n",
       "   'even',\n",
       "   'when',\n",
       "   'there',\n",
       "   'is',\n",
       "   'no',\n",
       "   'crisis',\n",
       "   '.'],),\n",
       " (['@sfgiantsfan55',\n",
       "   'omg',\n",
       "   'Todo',\n",
       "   'Cabio',\n",
       "   'has',\n",
       "   'been',\n",
       "   'my',\n",
       "   'fav',\n",
       "   'since',\n",
       "   'like',\n",
       "   '`.7',\n",
       "   'i',\n",
       "   'still',\n",
       "   'listen',\n",
       "   'to',\n",
       "   'their',\n",
       "   'CD',\n",
       "   'all',\n",
       "   'the',\n",
       "   'time',\n",
       "   'i',\n",
       "   'want',\n",
       "   'the',\n",
       "   'new',\n",
       "   'one',\n",
       "   '.',\n",
       "   'alejate',\n",
       "   'de',\n",
       "   'mi',\n",
       "   '...'],),\n",
       " (['Resources',\n",
       "   '-',\n",
       "   'Tweets',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Week',\n",
       "   '(',\n",
       "   '9.13',\n",
       "   '.',\n",
       "   '1.',\n",
       "   '-',\n",
       "   '9.17',\n",
       "   '.',\n",
       "   '1.)',\n",
       "   ':',\n",
       "   'This',\n",
       "   'weeks',\n",
       "   'Tweets',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Week',\n",
       "   'has',\n",
       "   'some',\n",
       "   'cool',\n",
       "   'inspiration',\n",
       "   ',',\n",
       "   'coupl',\n",
       "   '...',\n",
       "   'http://bit.ly/95X8G5'],),\n",
       " (['well',\n",
       "   'poolnewsfeed',\n",
       "   'when',\n",
       "   \"you're\",\n",
       "   'good',\n",
       "   ',',\n",
       "   \"you're\",\n",
       "   'good',\n",
       "   '!'],),\n",
       " (['@zeeDOTi',\n",
       "   'i',\n",
       "   'might',\n",
       "   'join',\n",
       "   'in',\n",
       "   'if',\n",
       "   'I',\n",
       "   'make',\n",
       "   'it',\n",
       "   'home',\n",
       "   'in',\n",
       "   'time',\n",
       "   '.',\n",
       "   ':)'],),\n",
       " (['When',\n",
       "   'TIFF',\n",
       "   'is',\n",
       "   'in',\n",
       "   'Tdot',\n",
       "   ',',\n",
       "   'ALWAYS',\n",
       "   'have',\n",
       "   'ur',\n",
       "   'camera',\n",
       "   'in',\n",
       "   'purse',\n",
       "   '!',\n",
       "   'Managed',\n",
       "   'to',\n",
       "   'get',\n",
       "   'a',\n",
       "   'pic',\n",
       "   'of',\n",
       "   'EVA',\n",
       "   'MENDES',\n",
       "   '!',\n",
       "   \"Didn't\",\n",
       "   'know',\n",
       "   'she',\n",
       "   'was',\n",
       "   'signing',\n",
       "   'autographs',\n",
       "   '@',\n",
       "   'the',\n",
       "   'mall',\n",
       "   '.'],),\n",
       " (['I',\n",
       "   'see',\n",
       "   'her',\n",
       "   'tomorrow',\n",
       "   ':',\n",
       "   '3',\n",
       "   'so',\n",
       "   'yeah~',\n",
       "   'ca',\n",
       "   \"n't\",\n",
       "   'wait'],),\n",
       " (['@singingislife15', 'Follow', 'Friday', ':)'],),\n",
       " (['@TheGayDragon',\n",
       "   '@RealmOfMachias',\n",
       "   'Sir',\n",
       "   '..',\n",
       "   'I',\n",
       "   'SWEAR',\n",
       "   'I',\n",
       "   'had',\n",
       "   'nothing',\n",
       "   'to',\n",
       "   'do',\n",
       "   'with',\n",
       "   'it',\n",
       "   'this',\n",
       "   'time',\n",
       "   '..',\n",
       "   'honest',\n",
       "   '..',\n",
       "   'please',\n",
       "   \"don't\",\n",
       "   'hurt',\n",
       "   'me',\n",
       "   '..'],),\n",
       " (['RT',\n",
       "   '@Quotealicious',\n",
       "   ':',\n",
       "   'When',\n",
       "   'ever',\n",
       "   'i',\n",
       "   'see',\n",
       "   'a',\n",
       "   'cute',\n",
       "   'love',\n",
       "   'story',\n",
       "   'movie',\n",
       "   ',',\n",
       "   'It',\n",
       "   \"'s\",\n",
       "   'always',\n",
       "   'you',\n",
       "   'that',\n",
       "   'comes',\n",
       "   'into',\n",
       "   'my',\n",
       "   'mind',\n",
       "   '#Quotealicious'],),\n",
       " (['Exclusive',\n",
       "   ':',\n",
       "   'Rep',\n",
       "   '.',\n",
       "   'Steve',\n",
       "   'King',\n",
       "   'on',\n",
       "   'ObamaCare',\n",
       "   ',',\n",
       "   'Tea',\n",
       "   'Party',\n",
       "   ',',\n",
       "   'and',\n",
       "   'Constitution',\n",
       "   'Day',\n",
       "   ':',\n",
       "   'The',\n",
       "   'inclusion',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Tenth',\n",
       "   'Amendment',\n",
       "   'in',\n",
       "   '...',\n",
       "   'http://bit.ly/cYITA8'],),\n",
       " (['@TimothyJMoore',\n",
       "   'Thnx',\n",
       "   ',',\n",
       "   \"I'm\",\n",
       "   'glad',\n",
       "   'they',\n",
       "   \"weren't\",\n",
       "   'totally',\n",
       "   'ignored',\n",
       "   ',',\n",
       "   'but',\n",
       "   'one',\n",
       "   \"isn't\",\n",
       "   'many',\n",
       "   'when',\n",
       "   'you',\n",
       "   'think',\n",
       "   'how',\n",
       "   'many',\n",
       "   'there',\n",
       "   'are',\n",
       "   'in',\n",
       "   'the',\n",
       "   'CofE',\n",
       "   '!'],),\n",
       " (['Justice',\n",
       "   'Breyer',\n",
       "   \"'s\",\n",
       "   'About',\n",
       "   'Face',\n",
       "   ':',\n",
       "   'Koran-Burning',\n",
       "   'Is',\n",
       "   'Constitutionally',\n",
       "   'Protected',\n",
       "   'After',\n",
       "   'All',\n",
       "   'via',\n",
       "   'Atlas',\n",
       "   'Shrugs',\n",
       "   'http://tinyurl.com/39wg73o'],),\n",
       " ([\"Don't\",\n",
       "   'forget',\n",
       "   'to',\n",
       "   'come',\n",
       "   'and',\n",
       "   'see',\n",
       "   'us',\n",
       "   'at',\n",
       "   'the',\n",
       "   'football',\n",
       "   'game',\n",
       "   'tomorrow',\n",
       "   'afternoon',\n",
       "   '!',\n",
       "   'Go',\n",
       "   'Reivers',\n",
       "   '!'],),\n",
       " (['...',\n",
       "   'asked',\n",
       "   'me',\n",
       "   'a',\n",
       "   'question',\n",
       "   'for',\n",
       "   'once',\n",
       "   ',',\n",
       "   'but',\n",
       "   'no',\n",
       "   '!',\n",
       "   'You',\n",
       "   'ask',\n",
       "   'me',\n",
       "   'about',\n",
       "   'fucking',\n",
       "   'donuts',\n",
       "   'you',\n",
       "   'donut',\n",
       "   '...',\n",
       "   'have',\n",
       "   'a',\n",
       "   'nice',\n",
       "   'day',\n",
       "   ':P'],),\n",
       " (['Here', 'we', 'go', ',', 'Friday', 'night', 'craziness'],),\n",
       " (['CHILLIN', '@LIVE', 'TONIGHT', '!'],),\n",
       " (['ugh',\n",
       "   'so',\n",
       "   'this',\n",
       "   'chick',\n",
       "   'onFB',\n",
       "   'jus',\n",
       "   'wrote',\n",
       "   'this',\n",
       "   'long',\n",
       "   'status',\n",
       "   'cussin',\n",
       "   'me',\n",
       "   'out',\n",
       "   ',',\n",
       "   'but',\n",
       "   'im',\n",
       "   'not',\n",
       "   'even',\n",
       "   'gonna',\n",
       "   'stoop',\n",
       "   'to',\n",
       "   'her',\n",
       "   'leve',\n",
       "   'but',\n",
       "   'i',\n",
       "   'wanna',\n",
       "   'bomb',\n",
       "   'her',\n",
       "   'ass',\n",
       "   'out',\n",
       "   'so',\n",
       "   'bad'],),\n",
       " (['today', 'just', \"doesn't\", 'feel', 'like', 'a', 'Friday'],),\n",
       " (['APO',\n",
       "   'disaster',\n",
       "   'relief',\n",
       "   'tomorrow',\n",
       "   '1.-12',\n",
       "   '.',\n",
       "   'Sign',\n",
       "   'up',\n",
       "   'on',\n",
       "   'the',\n",
       "   'website',\n",
       "   '!'],),\n",
       " (['Every',\n",
       "   'seven',\n",
       "   'years',\n",
       "   'you',\n",
       "   'lose',\n",
       "   'around',\n",
       "   'half',\n",
       "   'of',\n",
       "   'your',\n",
       "   'friends',\n",
       "   ',',\n",
       "   'and',\n",
       "   'replace',\n",
       "   'them',\n",
       "   'with',\n",
       "   'new',\n",
       "   'friends',\n",
       "   '.'],),\n",
       " (['RT',\n",
       "   '@rainnwilson',\n",
       "   ':',\n",
       "   'Hey',\n",
       "   ',',\n",
       "   'Seattle-ites',\n",
       "   '!',\n",
       "   'Check',\n",
       "   'it',\n",
       "   ':',\n",
       "   'http://www.officetally.com/rainn-wilson-and-friends-seattle-oct-23'],),\n",
       " (['For',\n",
       "   'code',\n",
       "   \"I've\",\n",
       "   'written',\n",
       "   'since',\n",
       "   'Sept',\n",
       "   \"'\",\n",
       "   '9',\n",
       "   ',',\n",
       "   'the',\n",
       "   'delay',\n",
       "   'between',\n",
       "   'commit',\n",
       "   'to',\n",
       "   'SCM',\n",
       "   'and',\n",
       "   'running',\n",
       "   'on',\n",
       "   'customer',\n",
       "   'hardware',\n",
       "   'has',\n",
       "   'been',\n",
       "   'about',\n",
       "   '1.',\n",
       "   'days',\n",
       "   '.'],),\n",
       " (['Balloon',\n",
       "   'glowing',\n",
       "   '?',\n",
       "   \"We'd\",\n",
       "   'love',\n",
       "   'to',\n",
       "   'see',\n",
       "   'you',\n",
       "   'before',\n",
       "   'or',\n",
       "   'after',\n",
       "   'at',\n",
       "   'Pi',\n",
       "   '!',\n",
       "   'CWE',\n",
       "   'and',\n",
       "   'Delmar',\n",
       "   'are',\n",
       "   'around',\n",
       "   'the',\n",
       "   'corner',\n",
       "   ',',\n",
       "   'Kirkwood',\n",
       "   '&amp;',\n",
       "   '...',\n",
       "   'http://fb.me/I6SZ52nW'],),\n",
       " (['All',\n",
       "   'caught',\n",
       "   'up',\n",
       "   'on',\n",
       "   '@SHO_weeds',\n",
       "   '!',\n",
       "   'After',\n",
       "   'eating',\n",
       "   'a',\n",
       "   'full',\n",
       "   'meal',\n",
       "   'that',\n",
       "   'was',\n",
       "   'the',\n",
       "   'next',\n",
       "   'thing',\n",
       "   'on',\n",
       "   'my',\n",
       "   'list',\n",
       "   'to',\n",
       "   'do',\n",
       "   'after',\n",
       "   'graduating',\n",
       "   '!'],),\n",
       " (['http://bit.ly/aTTQYq',\n",
       "   'When',\n",
       "   'Pepsi',\n",
       "   'to',\n",
       "   'ring',\n",
       "   'usually',\n",
       "   'confirm',\n",
       "   'to',\n",
       "   ',',\n",
       "   'winning',\n",
       "   'a',\n",
       "   'Nokia',\n",
       "   '58..',\n",
       "   '?'],),\n",
       " (['RT',\n",
       "   '@Slijterijmeisje',\n",
       "   ':',\n",
       "   'Kreeg',\n",
       "   'net',\n",
       "   'een',\n",
       "   'bruikbare',\n",
       "   'tip',\n",
       "   'van',\n",
       "   'iemand',\n",
       "   'die',\n",
       "   'vorige',\n",
       "   'week',\n",
       "   'was',\n",
       "   'begonnen',\n",
       "   'met',\n",
       "   'een',\n",
       "   'whiskydieet',\n",
       "   ',',\n",
       "   'hij',\n",
       "   'was',\n",
       "   'nu',\n",
       "   'al',\n",
       "   '3',\n",
       "   'dagen',\n",
       "   'kwijt'],),\n",
       " (['#news',\n",
       "   'Dems',\n",
       "   'to',\n",
       "   'voters',\n",
       "   ':',\n",
       "   'You',\n",
       "   'may',\n",
       "   'hate',\n",
       "   'us',\n",
       "   ',',\n",
       "   'but',\n",
       "   'GOP',\n",
       "   'is',\n",
       "   'worse',\n",
       "   '(',\n",
       "   'AP',\n",
       "   ')',\n",
       "   '(',\n",
       "   'Yahoo',\n",
       "   '!',\n",
       "   ')',\n",
       "   ':',\n",
       "   'Share',\n",
       "   'With',\n",
       "   'Friends',\n",
       "   ':',\n",
       "   '|',\n",
       "   'Latest',\n",
       "   'Top',\n",
       "   'Ne',\n",
       "   '...',\n",
       "   'http://adpro.co/aQxQtY'],),\n",
       " (['@snufflesgirl25',\n",
       "   '@mo9x',\n",
       "   'Well',\n",
       "   '.',\n",
       "   'just',\n",
       "   'beg',\n",
       "   '@1..monkeysmusic',\n",
       "   '..',\n",
       "   'PLEASE',\n",
       "   'come',\n",
       "   'to',\n",
       "   'the',\n",
       "   '#SBLeurope',\n",
       "   'Next',\n",
       "   'year',\n",
       "   '!.',\n",
       "   'We',\n",
       "   'need',\n",
       "   'you',\n",
       "   'there',\n",
       "   '!',\n",
       "   '*puss',\n",
       "   'in',\n",
       "   'boots',\n",
       "   'eyes*'],),\n",
       " (['RT',\n",
       "   '@DollarVanDemos',\n",
       "   ':',\n",
       "   '#BROOKLYN',\n",
       "   '!',\n",
       "   'Stay',\n",
       "   'up',\n",
       "   'late',\n",
       "   'tonight',\n",
       "   'to',\n",
       "   'catch',\n",
       "   'the',\n",
       "   '1',\n",
       "   'hr',\n",
       "   'special',\n",
       "   'of',\n",
       "   'Demos',\n",
       "   '@',\n",
       "   '3AM',\n",
       "   'on',\n",
       "   'BCAT',\n",
       "   'feat',\n",
       "   '@BlissyCakes',\n",
       "   '@YaGirlNicolette',\n",
       "   '...'],),\n",
       " (['ahhh',\n",
       "   'the',\n",
       "   'disapointment',\n",
       "   ',',\n",
       "   'after',\n",
       "   'all',\n",
       "   'them',\n",
       "   'awkward',\n",
       "   'bait',\n",
       "   'moments',\n",
       "   '.'],),\n",
       " (['CLUB',\n",
       "   'BLU',\n",
       "   'tonite',\n",
       "   '......',\n",
       "   '9.',\n",
       "   \"'s\",\n",
       "   'music',\n",
       "   '..',\n",
       "   'oldskool',\n",
       "   'night',\n",
       "   'wiith',\n",
       "   'dj',\n",
       "   'finese'],),\n",
       " (['ooo',\n",
       "   '@claireymarsh',\n",
       "   'where',\n",
       "   'are',\n",
       "   'you',\n",
       "   'when',\n",
       "   'i',\n",
       "   'need',\n",
       "   'you',\n",
       "   '..'],),\n",
       " (['@iDame2Please',\n",
       "   'yea',\n",
       "   'u',\n",
       "   'c',\n",
       "   'da',\n",
       "   'pic',\n",
       "   '1st',\n",
       "   'Presbyterian',\n",
       "   'all',\n",
       "   'day',\n",
       "   'lol'],),\n",
       " (['Time',\n",
       "   'to',\n",
       "   'go',\n",
       "   ',',\n",
       "   'hihi',\n",
       "   '!',\n",
       "   'Beijinhos',\n",
       "   ',',\n",
       "   'seus',\n",
       "   'viciados',\n",
       "   ':',\n",
       "   '*'],),\n",
       " (['RT',\n",
       "   '@rocsidiaz',\n",
       "   ':',\n",
       "   '@Wale',\n",
       "   'ill',\n",
       "   'be',\n",
       "   'in',\n",
       "   'DC',\n",
       "   'tonight',\n",
       "   'where',\n",
       "   'u',\n",
       "   'at',\n",
       "   '??',\n",
       "   'Ill',\n",
       "   'be',\n",
       "   'at',\n",
       "   'muse',\n",
       "   '717',\n",
       "   '6th',\n",
       "   'St',\n",
       "   'Nw',\n",
       "   'Dc',\n",
       "   'to',\n",
       "   'party'],),\n",
       " (['@LinnySmit',\n",
       "   'Linny',\n",
       "   'Linny',\n",
       "   'Linny',\n",
       "   '.',\n",
       "   'U',\n",
       "   'are',\n",
       "   'something',\n",
       "   'lady',\n",
       "   '...',\n",
       "   'Well',\n",
       "   'that',\n",
       "   'story',\n",
       "   'has',\n",
       "   'been',\n",
       "   'on',\n",
       "   'hold',\n",
       "   'for',\n",
       "   'weeks',\n",
       "   'now',\n",
       "   \"hasn't\",\n",
       "   'it',\n",
       "   '?',\n",
       "   'I',\n",
       "   'want',\n",
       "   'to',\n",
       "   'get',\n",
       "   'on',\n",
       "   'it',\n",
       "   'this',\n",
       "   '&gt;',\n",
       "   '&gt;'],),\n",
       " (['International',\n",
       "   'Observe',\n",
       "   'the',\n",
       "   'Moon',\n",
       "   'Night',\n",
       "   '(',\n",
       "   '9/18/2.1.',\n",
       "   ')',\n",
       "   '|',\n",
       "   'Saturday',\n",
       "   ',',\n",
       "   'Sep',\n",
       "   '18',\n",
       "   ',',\n",
       "   '.6:..',\n",
       "   'PM',\n",
       "   '|',\n",
       "   'http://bit.ly/bVOdBF',\n",
       "   '#cuboulder'],),\n",
       " (['waiting',\n",
       "   'one',\n",
       "   'extra',\n",
       "   'day',\n",
       "   'was',\n",
       "   'a',\n",
       "   'STUPID',\n",
       "   'IDEA',\n",
       "   'because',\n",
       "   'now',\n",
       "   'everything',\n",
       "   'is',\n",
       "   'super',\n",
       "   'expensive',\n",
       "   'AAAAAHHH',\n",
       "   '.',\n",
       "   'x_x',\n",
       "   '*freakingoutkhsf*'],),\n",
       " ([\"'Was\",\n",
       "   'going',\n",
       "   'to',\n",
       "   'stay',\n",
       "   'until',\n",
       "   'Sunday',\n",
       "   'but',\n",
       "   'my',\n",
       "   '\"',\n",
       "   'mother',\n",
       "   '\"',\n",
       "   'has',\n",
       "   'already',\n",
       "   'made',\n",
       "   'me',\n",
       "   'mad',\n",
       "   'furthermore',\n",
       "   'hair',\n",
       "   'appointment',\n",
       "   'tomrrw',\n",
       "   'and',\n",
       "   'back',\n",
       "   'to',\n",
       "   'Kent',\n",
       "   \"!'\"],)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_test_sentences = [[w if w in words_to_keep else 'UNK' for w in words[0]] for words in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_test_sentences = np.asarray(parsed_test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/btech/cse/2016/mukuntha.cs16/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:147: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "train_data = parsed_sentences\n",
    "test_data = np.asarray(parsed_test_sentences)\n",
    "X_train = [a[0] for a in train_data]\n",
    "Y_train = [a[1] for a in train_data]\n",
    "X_test = [a for a in test_data]\n",
    "# Y_test = [a[1] for a in test_data]\n",
    "\n",
    "# Build the vocabulary and word counts.\n",
    "vocabulary2id, tag2id = get_vocab(X_train, Y_train)\n",
    "wordcount, tagcount, tagpaircount, tagtriplecount = get_word_tag_counts(X_train, Y_train, vocabulary2id, tag2id)\n",
    "cwords, ctags, cpairs, ctrans, cinits, ctrans1 = make_counts(X_train, Y_train)\n",
    "\n",
    "state_list = list(ctags.keys())\n",
    "observation_list = [a[0] for a in sorted(vocabulary2id.items(), key = lambda x: x[1])]\n",
    "hmm = HMM(state_list = state_list, observation_list = observation_list,\n",
    "          transition_proba = None, observation_proba = None, initial_state_proba = None,\n",
    "          smoothing_obs = 0.4, prob_abs = 0)\n",
    "hmm.supervised_training(cpairs, ctrans, cinits, ctrans1)\n",
    "\n",
    "predictions = []\n",
    "for x in X_test:\n",
    "    for i in range(len(x)):\n",
    "        if x[i] not in vocabulary2id.keys():\n",
    "            x[i] = 'UNK'\n",
    "\n",
    "    pred_idx = hmm.viterbi(x)\n",
    "    y_pred = np.asarray([state_list[int(round(i))] for i in pred_idx])\n",
    "    predictions.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = [list(s) for s in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie', 'O'], ['O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-movie'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-product', 'I-product', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-other', 'I-other', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie'], ['O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie', 'I-movie', 'I-movie', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-product', 'I-product', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-product', 'I-product', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-movie', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-person', 'I-person', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-person', 'I-person', 'O', 'B-sportsteam', 'I-sportsteam', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie', 'I-movie'], ['O', 'O', 'B-tvshow', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-person', 'I-person', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie'], ['O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'B-movie', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-tvshow'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-facility', 'I-facility', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-product', 'I-product', 'I-product', 'I-product', 'I-product', 'I-product', 'I-product', 'I-product', 'I-product', 'I-product', 'I-product', 'I-product', 'I-product', 'I-product', 'I-product', 'I-product', 'I-product', 'I-product', 'I-product', 'I-product', 'I-product', 'I-product', 'I-product'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie', 'I-movie'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-company', 'I-company', 'I-company', 'B-geo-loc'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
