{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries.\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "import operator\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "random.seed(11)\n",
    "np.random.seed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NER-Dataset-10Types-Train.txt\r\n",
      " NER-Dataset--TestSet.txt\r\n",
      " NER-Dataset-Train.txt\r\n",
      " NER-TestSet-10Types-HMM-Predictions.txt\r\n",
      " NER-TestSet-HMM-Predictions.txt\r\n",
      "'Q1 - NER Prediction - 10 Types (HMM).ipynb'\r\n",
      "'Q1 - NER Prediction (HMM).ipynb'\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NER-Dataset-10Types-Train.txt', 'r') as f:\n",
    "    ner_dataset = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "words = []\n",
    "tags = []\n",
    "for line in ner_dataset:\n",
    "    line = line.strip()\n",
    "    if line == '':\n",
    "        sentences.append((words, tags))\n",
    "        words = []\n",
    "        tags = []\n",
    "    else:\n",
    "        word, tag = line.split('\\t')\n",
    "        words.append(word)\n",
    "        tags.append(tag)\n",
    "\n",
    "if len(words) > 0:\n",
    "    sentences.append((words, tags))\n",
    "    words = []\n",
    "    tags= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_counts = Counter(sum([a[0] for a in sentences], [])).most_common()\n",
    "words_to_keep = set([word for word, count in vocab_counts if count > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_sentences = [([w if w in words_to_keep else 'UNK' for w in words], tags) for words, tags in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(X_train, Y_train):\n",
    "    '''\n",
    "    Function for building the vocabulary from the training set of\n",
    "    words and tags.\n",
    "    '''\n",
    "    vocabulary2id = dict()    \n",
    "    tag2id = dict()\n",
    "    vocabulary2id['UNK'] = 0\n",
    "\n",
    "    for sent in X_train:\n",
    "        for word in sent:\n",
    "            if word not in vocabulary2id.keys():\n",
    "                vocabulary2id[word] = len(vocabulary2id)\n",
    "\n",
    "    for sent in Y_train:\n",
    "        for tag in sent:\n",
    "            if tag not in tag2id.keys():\n",
    "                tag2id[tag] = len(tag2id)\n",
    "\n",
    "    return vocabulary2id, tag2id\n",
    "\n",
    "def get_word_tag_counts(X_train, Y_train, vocabulary2id, tag2id):\n",
    "    '''\n",
    "    Function for calculating the counts pertaining to the\n",
    "    individual word tags.\n",
    "    '''\n",
    "    wordcount = defaultdict(int)\n",
    "    tagcount = defaultdict(int)\n",
    "    tagpaircount = defaultdict(int)\n",
    "    tagtriplecount = defaultdict(int)\n",
    "    \n",
    "    for sent in X_train:\n",
    "        for word in sent:\n",
    "            wordcount[word] += 1\n",
    "    \n",
    "    for sent in Y_train:\n",
    "        for tag in sent:\n",
    "            tagcount[tag] += 1\n",
    "    \n",
    "    for sent in Y_train:\n",
    "        for i in range(len(sent) - 1):\n",
    "            tagpaircount[sent[i], sent[i + 1]] += 1\n",
    "\n",
    "    for sent in Y_train:\n",
    "        for i in range(len(sent) - 2):\n",
    "            tagtriplecount[sent[i], sent[i + 1], sent[i + 2]] += 1\n",
    "    \n",
    "    return wordcount, tagcount, tagpaircount, tagtriplecount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token to map all out-of-vocabulary words (OOVs).\n",
    "UNK = \"UNK\"\n",
    "# Index for UNK\n",
    "UNKid = 0\n",
    "epsilon = 1e-100\n",
    "array, ones, zeros, multiply, unravel_index = np.array, np.ones, np.zeros, np.multiply, np.unravel_index\n",
    "\n",
    "class HMM:\n",
    "    def __init__(self, state_list, observation_list, transition_proba = None,\n",
    "                 observation_proba = None, initial_state_proba = None, \n",
    "                 smoothing_obs = 0.01, transition_proba1 = None, prob_abs = 0.00001):\n",
    "        '''\n",
    "        Builds a Hidden Markov Model.\n",
    "        * state_list is the list of state symbols [q_0...q_(N-1)]\n",
    "        * observation_list is the list of observation symbols [v_0...v_(M-1)]\n",
    "        * transition_proba is the transition probability matrix\n",
    "            [a_ij] a_ij,a_ik = Pr(Y_(t+1)=q_i|Y_t=q_j,Y_(t-1)=q_k)\n",
    "        * observation_proba is the observation probablility matrix\n",
    "            [b_ki] b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "        * initial_state_proba is the initial state distribution\n",
    "            [pi_i] pi_i = Pr(Y_0=q_i)\n",
    "        '''\n",
    "        # Number of states.\n",
    "        self.N = len(state_list)\n",
    "        # Number of possible emissions.\n",
    "        self.M = len(observation_list)\n",
    "        self.prob_abs = prob_abs\n",
    "        self.omega_Y = state_list\n",
    "        self.omega_X = observation_list\n",
    "\n",
    "        if transition_proba1 is None:\n",
    "            self.transition_proba1 = zeros( (self.N, self.N), float) \n",
    "        else:\n",
    "            self.transition_proba1 = transition_proba1\n",
    "\n",
    "        if transition_proba is None:\n",
    "            self.transition_proba = zeros( (self.N, self.N, self.N), float) \n",
    "        else:\n",
    "            self.transition_proba=transition_proba\n",
    "\n",
    "        if observation_proba is None:\n",
    "            self.observation_proba = zeros( (self.M, self.N), float) \n",
    "        else:\n",
    "            self.observation_proba = observation_proba\n",
    "\n",
    "        if initial_state_proba is None:\n",
    "            self.initial_state_proba = zeros( (self.N,), float ) \n",
    "        else:\n",
    "            self.initial_state_proba = initial_state_proba\n",
    "\n",
    "        # Build indexes, i.e., the mapping between token and int.\n",
    "        self.make_indexes()\n",
    "        self.smoothing_obs = smoothing_obs \n",
    "        \n",
    "    def make_indexes(self):\n",
    "        '''\n",
    "        Function for creating the reverse table that maps\n",
    "        states/observations names to their index in the probabilities\n",
    "        array.\n",
    "        '''\n",
    "        self.Y_index = {}\n",
    "\n",
    "        for i in range(self.N):\n",
    "            self.Y_index[self.omega_Y[i]] = i\n",
    "            \n",
    "        self.X_index = {}\n",
    "            \n",
    "        for i in range(self.M):\n",
    "            self.X_index[self.omega_X[i]] = i\n",
    "        \n",
    "    def get_observationIndices(self, observations):\n",
    "        '''\n",
    "        Function for returning observation indices,\n",
    "        and dealing with OOVs.\n",
    "        '''\n",
    "        indices = zeros( len(observations), int )\n",
    "        k = 0\n",
    "\n",
    "        for o in observations:\n",
    "            if o in self.X_index:\n",
    "                indices[k] = self.X_index[o]\n",
    "            else:\n",
    "                indices[k] = UNKid\n",
    "\n",
    "            k += 1\n",
    "\n",
    "        return indices\n",
    "\n",
    "    def data2indices(self, sent): \n",
    "        '''\n",
    "        Function for extracting the words and tags and returning a\n",
    "        list of indices for each.\n",
    "        '''\n",
    "        wordids = list()\n",
    "        tagids  = list()\n",
    "\n",
    "        for couple in sent:\n",
    "            wrd = couple[0]\n",
    "            tag = couple[1]\n",
    "\n",
    "            if wrd in self.X_index:\n",
    "                wordids.append(self.X_index[wrd])\n",
    "            else:\n",
    "                wordids.append(UNKid)\n",
    "\n",
    "            tagids.append(self.Y_index[tag])\n",
    "\n",
    "        return wordids, tagids\n",
    "        \n",
    "    def observation_estimation(self, pair_counts):\n",
    "        '''\n",
    "        Function for building the observation distribution where\n",
    "        observation_proba is the observation probablility matrix.\n",
    "        '''\n",
    "        # Fill with counts.\n",
    "        for pair in pair_counts:\n",
    "            wrd = pair[0]\n",
    "            tag = pair[1]\n",
    "            cpt = pair_counts[pair]\n",
    "            # For UNK.\n",
    "            k = 0\n",
    "\n",
    "            if wrd in self.X_index: \n",
    "                k = self.X_index[wrd]\n",
    "\n",
    "            i = self.Y_index[tag]\n",
    "            self.observation_proba[k, i] = cpt\n",
    "\n",
    "        # Normalize.\n",
    "        self.observation_proba = self.observation_proba + self.smoothing_obs\n",
    "        self.observation_proba = self.observation_proba / self.observation_proba.sum(axis = 0).reshape(1, self.N)\n",
    "\n",
    "    def transition_estimation(self, trans_counts):\n",
    "        '''\n",
    "        Function for building the transition distribution where \n",
    "        transition_proba is the transition matrix with:\n",
    "        [a_ij] a[i, j] = Pr(Y_(t+1) = q_i | Y_t = q_j, Y_(t-1) = q_k)\n",
    "        '''\n",
    "        # Fill with counts.\n",
    "        for triple in trans_counts:\n",
    "            i = self.Y_index[triple[2]]\n",
    "            j = self.Y_index[triple[1]]\n",
    "            k = self.Y_index[triple[0]]\n",
    "            self.transition_proba[k, j, i] = trans_counts[triple]\n",
    "\n",
    "        # Normalize.\n",
    "        self.transition_proba = self.transition_proba / self.transition_proba.sum(axis = 0).reshape(self.N, self.N)\n",
    "\n",
    "    def transition_estimation1(self, trans_counts):\n",
    "        '''\n",
    "        Function for building the transition distribution where \n",
    "        transition_proba is the transition matrix with: \n",
    "        [a_ij] a[i,j] = Pr(Y_(t+1)=q_i|Y_t=q_j)\n",
    "        '''\n",
    "        # Fill with counts.\n",
    "        for pair in trans_counts:\n",
    "            i = self.Y_index[pair[1]]\n",
    "            j = self.Y_index[pair[0]]\n",
    "            self.transition_proba1[j, i] = trans_counts[pair]\n",
    "\n",
    "        # Normalize.\n",
    "        self.transition_proba1 = self.transition_proba1 / self.transition_proba1.sum(axis = 0).reshape(1, self.N)\n",
    "        \n",
    "    def init_estimation(self, init_counts):\n",
    "        '''\n",
    "        Function for building the initial distribution.\n",
    "        '''\n",
    "        # Fill with counts.\n",
    "        for tag in init_counts:\n",
    "            i = self.Y_index[tag]\n",
    "            self.initial_state_proba[i] = init_counts[tag]\n",
    "\n",
    "        # Normalize.\n",
    "        self.initial_state_proba = self.initial_state_proba / sum(self.initial_state_proba)\n",
    "\n",
    "    def supervised_training(self, pair_counts, trans_counts, init_counts, trans_counts1):\n",
    "        '''\n",
    "        Function for training the HMM's parameters.\n",
    "        '''\n",
    "        self.observation_estimation(pair_counts)\n",
    "        self.transition_estimation(trans_counts)\n",
    "        self.transition_estimation1(trans_counts1)\n",
    "        self.init_estimation(init_counts)\n",
    "        \n",
    "    def viterbi(self, observations):\n",
    "        if len(observations) < 2:\n",
    "            return [np.argmax(hmm.observation_proba[hmm.X_index[word]]) for z in observations]\n",
    "\n",
    "        nSamples = len(observations)\n",
    "        # Number of states.\n",
    "        nStates = self.transition_proba.shape[0]\n",
    "        # Scale factors (necessary to prevent underflow).\n",
    "        c = np.zeros(nSamples)\n",
    "        # Initialise viterbi table.\n",
    "        viterbi = np.zeros((nStates, nStates, nSamples))\n",
    "        # Initialise viterbi table.\n",
    "        viterbi1 = np.zeros((nStates, nSamples))\n",
    "        # Initialise the best path table.\n",
    "        psi = np.zeros((nStates, nStates, nSamples))\n",
    "        best_path = np.zeros(nSamples)\n",
    "        idx0 = self.X_index[observations[0]]\n",
    "        idx1 = self.X_index[observations[1]]\n",
    "        viterbi1[:, 0] = self.initial_state_proba.T * self.observation_proba[idx0, :].T\n",
    "\n",
    "        # Loop through the states.\n",
    "        for s in range (0, nStates):\n",
    "            for v in range (0, nStates):\n",
    "                viterbi[s, v, 1] = viterbi1[s, 0] * self.transition_proba1[s, v] * self.observation_proba[idx1, v]\n",
    "\n",
    "        psi[0] = 0;\n",
    "\n",
    "        # Loop through time-stamps.\n",
    "        for t in range(2, nSamples):\n",
    "            idx = self.X_index[observations[t]]\n",
    "            # Loop through the states.\n",
    "            for s in range (0, nStates):\n",
    "                for v in range (0, nStates):\n",
    "                    self.transition_proba[np.isnan(self.transition_proba)] = self.prob_abs\n",
    "                    trans_p = viterbi[:, s, t-1] * self.transition_proba[:, s, v]\n",
    "\n",
    "                    if (math.isnan(trans_p[0])):\n",
    "                        trans_p[0] = 0\n",
    "\n",
    "                    psi[s, v, t], viterbi[s, v, t] = max(enumerate(trans_p), key = operator.itemgetter(1))\n",
    "                    viterbi[s, v, t] = viterbi[s, v, t] * self.observation_proba[idx, v]\n",
    "\n",
    "        cabbar = viterbi[:, :, nSamples - 1]\n",
    "        best_path[nSamples - 1] = unravel_index(cabbar.argmax(), cabbar.shape)[1]\n",
    "        best_path[nSamples - 2] = unravel_index(cabbar.argmax(), cabbar.shape)[0]\n",
    "\n",
    "        # Return the best path, number of samples and psi.\n",
    "        for t in range(nSamples - 3, -1, -1):\n",
    "            best_path[t] = psi[int(round(best_path[t + 1])), int(round(best_path[t + 2])), t + 2]\n",
    "\n",
    "        return best_path\n",
    "        \n",
    "    def fwd_bkw(self, observations):\n",
    "        observations = x\n",
    "        self = hmm\n",
    "        nStates = self.transition_proba.shape[0]\n",
    "        start_prob = self.initial_state_proba\n",
    "        trans_prob = self.transition_proba1.transpose()\n",
    "        emm_prob = self.observation_proba.transpose()\n",
    "\n",
    "        # Forward part of the algorithm.\n",
    "        fwd = []\n",
    "        f_prev = {}\n",
    "\n",
    "        for i, observation_i in enumerate(observations):\n",
    "            f_curr = {}\n",
    "            for st in range(nStates):\n",
    "                if i == 0:\n",
    "                    # Base case for the forward part.\n",
    "                    prev_f_sum = start_prob[st]\n",
    "                else:\n",
    "                    prev_f_sum = sum(f_prev[k] * trans_prob[k][st] for k in range(nStates))\n",
    "\n",
    "                f_curr[st] = emm_prob[st][self.X_index[observation_i]] * prev_f_sum\n",
    "\n",
    "            fwd.append(f_curr)\n",
    "            f_prev = f_curr\n",
    "\n",
    "        p_fwd = sum(f_curr[k] for k in range(nStates))\n",
    "\n",
    "        # Backward part of the algorithm.\n",
    "        bkw = []\n",
    "        b_prev = {}\n",
    "\n",
    "        for i, observation_i_plus in enumerate(reversed(observations[1:] + [None,])):\n",
    "            b_curr = {}\n",
    "            for st in range(nStates):\n",
    "                if i == 0:\n",
    "                    # Base case for backward part.\n",
    "                    b_curr[st] = 1.0\n",
    "                else:\n",
    "                    b_curr[st] = sum(trans_prob[st][l] * emm_prob[l][self.X_index[observation_i_plus]]\n",
    "                                     * b_prev[l] for l in range(nStates))\n",
    "\n",
    "            bkw.insert(0,b_curr)\n",
    "            b_prev = b_curr\n",
    "\n",
    "        p_bkw = sum(start_prob[l] * emm_prob[l][self.X_index[observations[0]]] * b_curr[l] for l in range(nStates))\n",
    "\n",
    "        # Merging the two parts.\n",
    "        posterior = []\n",
    "\n",
    "        for i in range(len(observations)):\n",
    "            posterior.append({st: fwd[i][st] * bkw[i][st] / p_fwd for st in range(nStates)})\n",
    "\n",
    "        assert abs(p_fwd - p_bkw) < 1e-6\n",
    "        return fwd, bkw, posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_counts(X, Y):\n",
    "    ''' \n",
    "    Function for building the different count tables to train a HMM.\n",
    "    Each count table is a dictionary. \n",
    "    '''\n",
    "    c_words = dict()\n",
    "    c_tags = dict()\n",
    "    c_pairs= dict()\n",
    "    c_transitions = dict()\n",
    "    c_inits = dict()\n",
    "    c_transitions1 = dict()\n",
    "    \n",
    "    for sent in zip(X, Y):\n",
    "        sent = list(zip(*sent))\n",
    "        for i in range(len(sent)):\n",
    "            couple = sent[i]\n",
    "            wrd = couple[0]\n",
    "            tag = couple[1]\n",
    "\n",
    "            # Word counts.\n",
    "            if wrd in c_words:\n",
    "                c_words[wrd] = c_words[wrd] + 1\n",
    "            else:\n",
    "                c_words[wrd] = 1\n",
    "\n",
    "            # Tag counts.\n",
    "            if tag in c_tags:\n",
    "                c_tags[tag] = c_tags[tag] + 1\n",
    "            else:\n",
    "                c_tags[tag] = 1\n",
    "\n",
    "            # Observation counts.\n",
    "            if couple in c_pairs:\n",
    "                c_pairs[couple] = c_pairs[couple] + 1\n",
    "            else:\n",
    "                c_pairs[couple] = 1\n",
    "\n",
    "            if i >= 1:\n",
    "                trans1 = (sent[i - 1][1], tag)\n",
    "                if trans1 in c_transitions1:\n",
    "                    c_transitions1[trans1] = c_transitions1[trans1] + 1\n",
    "                else:\n",
    "                    c_transitions1[trans1] = 1\n",
    "\n",
    "            if i > 1:\n",
    "                trans = (sent[i - 2][1], sent[i - 1][1], tag)\n",
    "                if trans in c_transitions:\n",
    "                    c_transitions[trans] = c_transitions[trans] + 1\n",
    "                else:\n",
    "                    c_transitions[trans] = 1\n",
    "            else:\n",
    "                if tag in c_inits:\n",
    "                    c_inits[tag] = c_inits[tag] + 1\n",
    "                else:\n",
    "                    c_inits[tag] = 1\n",
    "    \n",
    "    return c_words, c_tags, c_pairs, c_transitions, c_inits, c_transitions1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shikhar/anaconda3/envs/keras/lib/python3.7/site-packages/ipykernel_launcher.py:147: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Accuracy and Scores:\n",
      "Viterbi Accuracy: 1.0, Precision: 1.0, Recall: 1.0, FScore: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shikhar/anaconda3/envs/keras/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Accuracy and Scores:\n",
      "Viterbi Accuracy: 0.9615384615384616, Precision: 0.4807692307692308, Recall: 0.5, FScore: 0.49019607843137253\n",
      "Fold 3 Accuracy and Scores:\n",
      "Viterbi Accuracy: 0.8, Precision: 0.26666666666666666, Recall: 0.3333333333333333, FScore: 0.29629629629629634\n",
      "Fold 4 Accuracy and Scores:\n",
      "Viterbi Accuracy: 1.0, Precision: 1.0, Recall: 1.0, FScore: 1.0\n",
      "Fold 5 Accuracy and Scores:\n",
      "Viterbi Accuracy: 0.8888888888888888, Precision: 0.2222222222222222, Recall: 0.25, FScore: 0.23529411764705882\n",
      "Overall Accuracy and Scores:\n",
      "Viterbi Accuracy: 0.9012013729977116, Precision: 0.17299751704223215, Recall: 0.1306308711792958, FScore: 0.1416866524575724\n"
     ]
    }
   ],
   "source": [
    "# Build the test and training sets of sentences.\n",
    "kf = KFold(n_splits = 5, shuffle = False)\n",
    "parsed_sentences = np.asarray(parsed_sentences)\n",
    "scores = []\n",
    "scores1 = []\n",
    "y_pred_idx = []\n",
    "y_pred_idx1 = []\n",
    "y_test_idx = []\n",
    "y_test_idx1 = []\n",
    "preds = []\n",
    "b = 0\n",
    "\n",
    "for train_index, test_index in kf.split(parsed_sentences):\n",
    "    b += 1\n",
    "    train_data = parsed_sentences[train_index]\n",
    "    test_data = parsed_sentences[test_index]\n",
    "    X_train = [a[0] for a in train_data]\n",
    "    Y_train = [a[1] for a in train_data]\n",
    "    X_test = [a[0] for a in test_data]\n",
    "    Y_test = [a[1] for a in test_data]\n",
    "    \n",
    "    # Build the vocabulary and word counts.\n",
    "    vocabulary2id, tag2id = get_vocab(X_train, Y_train)\n",
    "    wordcount, tagcount, tagpaircount, tagtriplecount = get_word_tag_counts(X_train, Y_train, vocabulary2id, tag2id)\n",
    "    cwords, ctags, cpairs, ctrans, cinits, ctrans1 = make_counts(X_train, Y_train)\n",
    "    \n",
    "    state_list = list(ctags.keys())\n",
    "    observation_list = [a[0] for a in sorted(vocabulary2id.items(), key = lambda x: x[1])]\n",
    "    hmm = HMM(state_list = state_list, observation_list = observation_list,\n",
    "              transition_proba = None, observation_proba = None, initial_state_proba = None,\n",
    "              smoothing_obs = 0.4, prob_abs = 0)\n",
    "    hmm.supervised_training(cpairs, ctrans, cinits, ctrans1)\n",
    "\n",
    "    for x, y_true in zip(X_test, Y_test):\n",
    "        for i in range(len(x)):\n",
    "            if x[i] not in vocabulary2id.keys():\n",
    "                x[i] = 'UNK'\n",
    "\n",
    "        pred_idx = hmm.viterbi(x)\n",
    "        y_pred = np.asarray([state_list[int(round(i))] for i in pred_idx])\n",
    "        y_true = np.asarray(y_true)\n",
    "        y_pred_idx += np.asarray([tag2id[lab] for lab in y_pred], dtype = np.int32).tolist()\n",
    "        y_test_idx += np.asarray([tag2id[lab] for lab in y_true], dtype = np.int32).tolist()\n",
    "        scores += (y_pred == y_true).tolist()\n",
    "\n",
    "    x, y_true = X_train[0], Y_train[0]\n",
    "\n",
    "    for x, y_true in zip(X_test, Y_test):\n",
    "        for i in range(len(x)):\n",
    "            if x[i] not in vocabulary2id.keys():\n",
    "                x[i] = 'UNK'\n",
    "\n",
    "        pred_probs = hmm.fwd_bkw(x)\n",
    "        pred_idx = [max(probs.items(), key=lambda x: x[1])[0] for probs in pred_probs[2]]\n",
    "        y_pred = np.asarray([state_list[int(round(i))] for i in pred_idx])\n",
    "        y_true = np.asarray(y_true)\n",
    "        y_pred_idx1 += np.asarray([tag2id[lab] for lab in y_pred], dtype = np.int32).tolist()\n",
    "        y_test_idx1 += np.asarray([tag2id[lab] for lab in y_true], dtype = np.int32).tolist()\n",
    "        scores1 += (y_pred == y_true).tolist()\n",
    "    \n",
    "    prec, rec, fscore, _ = precision_recall_fscore_support(np.asarray([tag2id[lab] for lab in y_true], dtype = np.int32).tolist(),\n",
    "                                                           np.asarray([tag2id[lab] for lab in y_pred], dtype = np.int32).tolist(),\n",
    "                                                           average = 'macro')\n",
    "    print('Fold ' + str(b) + ' Accuracy and Scores:')\n",
    "    print('Viterbi Accuracy: {}, Precision: {}, Recall: {}, FScore: {}'.format(np.asarray((y_pred == y_true).tolist()).mean(), prec, rec, fscore))\n",
    "\n",
    "prec, rec, fscore, _ = precision_recall_fscore_support(y_test_idx, y_pred_idx, average = 'macro')\n",
    "\n",
    "print('Overall Accuracy and Scores:')\n",
    "print('Viterbi Accuracy: {}, Precision: {}, Recall: {}, FScore: {}'.format(np.asarray(scores).mean(), prec, rec, fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NER-Dataset--TestSet.txt', 'r') as f:\n",
    "    test_dataset = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = []\n",
    "words = []\n",
    "for line in test_dataset:\n",
    "    line = line.strip()\n",
    "    if line == '':\n",
    "        test_sentences.append((words,))\n",
    "        words = []\n",
    "    else:\n",
    "        word = line\n",
    "        words.append(word)\n",
    "\n",
    "if len(words) > 0:\n",
    "    test_sentences.append((words,))\n",
    "    words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_test_sentences = [[w if w in words_to_keep else 'UNK' for w in words[0]] for words in test_sentences]\n",
    "parsed_test_sentences = np.asarray(parsed_test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shikhar/anaconda3/envs/keras/lib/python3.7/site-packages/ipykernel_launcher.py:147: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "train_data = parsed_sentences\n",
    "test_data = np.asarray(parsed_test_sentences)\n",
    "X_train = [a[0] for a in train_data]\n",
    "Y_train = [a[1] for a in train_data]\n",
    "X_test = [a for a in test_data]\n",
    "\n",
    "# Build the vocabulary and word counts.\n",
    "vocabulary2id, tag2id = get_vocab(X_train, Y_train)\n",
    "wordcount, tagcount, tagpaircount, tagtriplecount = get_word_tag_counts(X_train, Y_train, vocabulary2id, tag2id)\n",
    "cwords, ctags, cpairs, ctrans, cinits, ctrans1 = make_counts(X_train, Y_train)\n",
    "\n",
    "state_list = list(ctags.keys())\n",
    "observation_list = [a[0] for a in sorted(vocabulary2id.items(), key = lambda x: x[1])]\n",
    "hmm = HMM(state_list = state_list, observation_list = observation_list,\n",
    "          transition_proba = None, observation_proba = None, initial_state_proba = None,\n",
    "          smoothing_obs = 0.4, prob_abs = 0)\n",
    "hmm.supervised_training(cpairs, ctrans, cinits, ctrans1)\n",
    "\n",
    "predictions = []\n",
    "for x in X_test:\n",
    "    for i in range(len(x)):\n",
    "        if x[i] not in vocabulary2id.keys():\n",
    "            x[i] = 'UNK'\n",
    "\n",
    "    pred_idx = hmm.viterbi(x)\n",
    "    y_pred = np.asarray([state_list[int(round(i))] for i in pred_idx])\n",
    "    predictions.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = [list(s) for s in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NER-TestSet-10Types-HMM-Predictions.txt', 'w', encoding = 'utf-8') as f:\n",
    "    for words, predictions in zip(test_sentences, test_predictions):\n",
    "        assert(len(words[0]) == len(predictions))\n",
    "        for word, prediction in zip(words[0], predictions):\n",
    "            f.writelines(word + '\\t' + prediction + '\\n')\n",
    "        f.writelines('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
