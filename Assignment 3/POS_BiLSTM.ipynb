{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Primitive BiLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTAEuTboNeJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the required libraries.\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import collections\n",
        "import operator\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "\n",
        "random.seed(11)\n",
        "np.random.seed(11)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq2bZOCyNeKD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1871744f-39bf-4e7f-d677-050bf3f62711"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Conv1D, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation, Masking\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF8Th5RcOAka",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "287ace24-96ac-4a24-9254-25e912b8f4d7"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-96385671-2a64-453e-bc05-60d0fc86eecb\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-96385671-2a64-453e-bc05-60d0fc86eecb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Brown_train.txt to Brown_train (1).txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkPYCVj_NeKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_sentence(sentence):\n",
        "    '''\n",
        "    Function for parsing the words and tags from the\n",
        "    sentences of the input corpus.\n",
        "    '''\n",
        "    word_tag_pairs = sentence.split(\" \")\n",
        "    words = []\n",
        "    tags = []\n",
        "\n",
        "    for i, word_tag in enumerate(word_tag_pairs):\n",
        "        word, tag = word_tag.strip().rsplit('/', 1)\n",
        "        words.append(word)\n",
        "        tags.append(tag)\n",
        "        \n",
        "    return words, tags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXUd2ekoNeKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parse the sentences into a list.\n",
        "parsed_sentences = []\n",
        "\n",
        "with open('./Brown_train.txt', 'r') as file:\n",
        "    sentences = file.readlines()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.strip()\n",
        "        parsed_sentences.append(parse_sentence(sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwyuAfjGNeKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vocab(X_train, Y_train):\n",
        "    '''\n",
        "    Function for building the vocabulary from the training set of\n",
        "    words and tags.\n",
        "    '''\n",
        "    vocabulary2id = dict()    \n",
        "    tag2id = dict()\n",
        "    vocabulary2id['PAD'] = 0\n",
        "    vocabulary2id['UNK'] = 1\n",
        "\n",
        "    for sent in X_train:\n",
        "        for word in sent:\n",
        "            if word not in vocabulary2id.keys():\n",
        "                vocabulary2id[word] = len(vocabulary2id)\n",
        "    \n",
        "    tag2id['PAD'] = 0\n",
        "    for sent in Y_train:\n",
        "        for tag in sent:\n",
        "            if tag not in tag2id.keys():\n",
        "                tag2id[tag] = len(tag2id)\n",
        "    \n",
        "    return vocabulary2id, tag2id\n",
        "\n",
        "def get_word_tag_counts(X_train, Y_train, vocabulary2id, tag2id):\n",
        "    '''\n",
        "    Function for calculating the counts pertaining to the\n",
        "    individual word tags.\n",
        "    '''\n",
        "    wordcount = defaultdict(int)\n",
        "    tagcount = defaultdict(int)\n",
        "    tagpaircount = defaultdict(int)\n",
        "    tagtriplecount = defaultdict(int)\n",
        "    \n",
        "    for sent in X_train:\n",
        "        for word in sent:\n",
        "            wordcount[word] += 1\n",
        "    \n",
        "    for sent in Y_train:\n",
        "        for tag in sent:\n",
        "            tagcount[tag] += 1\n",
        "    \n",
        "    for sent in Y_train:\n",
        "        for i in range(len(sent) - 1):\n",
        "            tagpaircount[sent[i], sent[i + 1]] += 1\n",
        "\n",
        "    for sent in Y_train:\n",
        "        for i in range(len(sent) - 2):\n",
        "            tagtriplecount[sent[i], sent[i + 1], sent[i + 2]] += 1\n",
        "    \n",
        "    return wordcount, tagcount, tagpaircount, tagtriplecount"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtYOouggNeK0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3fe60c2b-3cef-4f94-c928-9cb7094e7654"
      },
      "source": [
        "parsed_sentences[:5]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['At',\n",
              "   'that',\n",
              "   'time',\n",
              "   'highway',\n",
              "   'engineers',\n",
              "   'traveled',\n",
              "   'rough',\n",
              "   'and',\n",
              "   'dirty',\n",
              "   'roads',\n",
              "   'to',\n",
              "   'accomplish',\n",
              "   'their',\n",
              "   'duties',\n",
              "   '.'],\n",
              "  ['ADP',\n",
              "   'DET',\n",
              "   'NOUN',\n",
              "   'NOUN',\n",
              "   'NOUN',\n",
              "   'VERB',\n",
              "   'ADJ',\n",
              "   'CONJ',\n",
              "   'ADJ',\n",
              "   'NOUN',\n",
              "   'PRT',\n",
              "   'VERB',\n",
              "   'DET',\n",
              "   'NOUN',\n",
              "   '.']),\n",
              " (['Using',\n",
              "   'privately-owned',\n",
              "   'vehicles',\n",
              "   'was',\n",
              "   'a',\n",
              "   'personal',\n",
              "   'hardship',\n",
              "   'for',\n",
              "   'such',\n",
              "   'employees',\n",
              "   ',',\n",
              "   'and',\n",
              "   'the',\n",
              "   'matter',\n",
              "   'of',\n",
              "   'providing',\n",
              "   'state',\n",
              "   'transportation',\n",
              "   'was',\n",
              "   'felt',\n",
              "   'perfectly',\n",
              "   'justifiable',\n",
              "   '.'],\n",
              "  ['VERB',\n",
              "   'ADJ',\n",
              "   'NOUN',\n",
              "   'VERB',\n",
              "   'DET',\n",
              "   'ADJ',\n",
              "   'NOUN',\n",
              "   'ADP',\n",
              "   'ADJ',\n",
              "   'NOUN',\n",
              "   '.',\n",
              "   'CONJ',\n",
              "   'DET',\n",
              "   'NOUN',\n",
              "   'ADP',\n",
              "   'VERB',\n",
              "   'NOUN',\n",
              "   'NOUN',\n",
              "   'VERB',\n",
              "   'VERB',\n",
              "   'ADV',\n",
              "   'ADJ',\n",
              "   '.']),\n",
              " (['Once',\n",
              "   'the',\n",
              "   'principle',\n",
              "   'was',\n",
              "   'established',\n",
              "   ',',\n",
              "   'the',\n",
              "   'increase',\n",
              "   'in',\n",
              "   'state-owned',\n",
              "   'vehicles',\n",
              "   'came',\n",
              "   'rapidly',\n",
              "   '.'],\n",
              "  ['ADP',\n",
              "   'DET',\n",
              "   'NOUN',\n",
              "   'VERB',\n",
              "   'VERB',\n",
              "   '.',\n",
              "   'DET',\n",
              "   'NOUN',\n",
              "   'ADP',\n",
              "   'ADJ',\n",
              "   'NOUN',\n",
              "   'VERB',\n",
              "   'ADV',\n",
              "   '.']),\n",
              " (['And',\n",
              "   'reasons',\n",
              "   'other',\n",
              "   'than',\n",
              "   'employee',\n",
              "   'need',\n",
              "   'contributed',\n",
              "   'to',\n",
              "   'the',\n",
              "   'growth',\n",
              "   '.'],\n",
              "  ['CONJ',\n",
              "   'NOUN',\n",
              "   'ADJ',\n",
              "   'ADP',\n",
              "   'NOUN',\n",
              "   'NOUN',\n",
              "   'VERB',\n",
              "   'ADP',\n",
              "   'DET',\n",
              "   'NOUN',\n",
              "   '.']),\n",
              " (['Table',\n",
              "   '1',\n",
              "   'immediately',\n",
              "   'below',\n",
              "   'shows',\n",
              "   'the',\n",
              "   'rate',\n",
              "   'of',\n",
              "   'growth',\n",
              "   'of',\n",
              "   'vehicles',\n",
              "   'and',\n",
              "   'employees',\n",
              "   '.'],\n",
              "  ['NOUN',\n",
              "   'NUM',\n",
              "   'ADV',\n",
              "   'ADV',\n",
              "   'VERB',\n",
              "   'DET',\n",
              "   'NOUN',\n",
              "   'ADP',\n",
              "   'NOUN',\n",
              "   'ADP',\n",
              "   'NOUN',\n",
              "   'CONJ',\n",
              "   'NOUN',\n",
              "   '.'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7JLr5AedooV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "    model = Sequential()\n",
        "    # model.add(InputLayer(input_shape=(padlen, )))\n",
        "    model.add(Masking(mask_value=float(vocabulary2id['UNK']),input_shape=(padlen,)))\n",
        "    model.add(Embedding(len(vocabulary2id), 100))\n",
        "    model.add(Bidirectional(LSTM(int((128+256)/2), return_sequences=True)))\n",
        "    model.add(TimeDistributed(Dense(len(tag2id))))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=Adam(0.001),\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeP15prFd7m_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def id2onehot(Y, numtags):\n",
        "    out = []\n",
        "    for s in Y:\n",
        "        categories = []\n",
        "        for item in s:\n",
        "            categories.append(np.zeros(numtags))\n",
        "            categories[-1][item] = 1.0\n",
        "        out.append(categories)\n",
        "    return np.array(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMo1HI3CNeK_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02959e63-da62-459b-fc07-cbc7c73bb403"
      },
      "source": [
        "# Build the test and training sets of sentences.\n",
        "kf = KFold(n_splits = 3, shuffle = False)\n",
        "parsed_sentences = np.asarray(parsed_sentences)\n",
        "scores = []\n",
        "scores1 = []\n",
        "y_pred_idx = []\n",
        "y_pred_idx1 = []\n",
        "y_test_idx = []\n",
        "y_test_idx1 = []\n",
        "\n",
        "preds_all_folds = []\n",
        "golds_all_folds = []\n",
        "\n",
        "for fold_num, (train_index, test_index) in enumerate(kf.split(parsed_sentences)):\n",
        "    train_data = parsed_sentences[train_index]\n",
        "    test_data = parsed_sentences[test_index]\n",
        "    X_train = [a[0] for a in train_data]\n",
        "    Y_train = [a[1] for a in train_data]\n",
        "    X_test = [a[0] for a in test_data]\n",
        "    Y_test = [a[1] for a in test_data]\n",
        "    \n",
        "    # Build the vocabulary and word counts.\n",
        "    vocabulary2id, tag2id = get_vocab(X_train, Y_train)\n",
        "    \n",
        "    padlen = max(len(i) for i in X_train)\n",
        "    def pad(sentence, padid=vocabulary2id['PAD']):\n",
        "        out = sentence[:padlen]\n",
        "        padding = [padid for _ in range(padlen - len(out))]\n",
        "        return out + padding\n",
        "    \n",
        "    X_train_ids = np.asarray([pad([vocabulary2id[word] if word in vocabulary2id.keys() else vocabulary2id['UNK'] for word in sent]) for sent in X_train])\n",
        "    X_test_ids = np.array([pad([vocabulary2id[word] if word in vocabulary2id.keys() else vocabulary2id['UNK'] for word in sent]) for sent in X_test])\n",
        "\n",
        "    Y_train_ids = np.asarray([pad([tag2id[word] if word in tag2id.keys() else tag2id['UNK'] for word in sent], tag2id['PAD']) for sent in Y_train])\n",
        "    Y_test_ids = np.asarray([pad([tag2id[word] if word in tag2id.keys() else tag2id['UNK'] for word in sent], tag2id['PAD']) for sent in Y_test])\n",
        "\n",
        "    Y_train_onehot = id2onehot(Y_train_ids, len(tag2id))\n",
        "    Y_test_onehot = id2onehot(Y_test_ids, len(tag2id))\n",
        "\n",
        "    model = build_model()\n",
        "    model.fit(X_train_ids, Y_train_onehot, batch_size=128, epochs=5, validation_split=0.2)\n",
        "\n",
        "    predictions = model.predict(X_test_ids)\n",
        "\n",
        "    # test_accuracy = np.sum((Y_test_ids == np.argmax(predictions, axis=-1)) * (Y_test_ids != 0)) / np.sum((Y_test_ids != 0))\n",
        "    # print('Fold {} test_accuracy: {}'.format(fold_num + 1, test_accuracy))\n",
        "\n",
        "    predictions_argmax = np.argmax(predictions, axis=-1)\n",
        "\n",
        "    y_pred_nopad = []\n",
        "    y_true_nopad = []\n",
        "\n",
        "    for i in range(len(Y_test_ids)):\n",
        "        for j in range(len(Y_test_ids[i])):\n",
        "            if Y_test_ids[i][j] != 0 and predictions_argmax[i][j] != 0:\n",
        "                y_true_nopad.append(Y_test_ids[i][j])\n",
        "                if predictions_argmax[i][j] == 0:\n",
        "                    y_pred_nopad.append(1)\n",
        "                else:\n",
        "                    y_pred_nopad.append(predictions_argmax[i][j])\n",
        "\n",
        "    preds_all_folds.extend(y_pred_nopad)\n",
        "    golds_all_folds.extend(y_true_nopad)\n",
        "\n",
        "    y_pred_nopad = np.asarray(y_pred_nopad)\n",
        "    y_true_nopad = np.asarray(y_true_nopad)\n",
        "    test_accuracy = (y_pred_nopad == y_true_nopad).mean()\n",
        "    print('Fold {} test_accuracy: {}'.format(fold_num + 1, test_accuracy))\n",
        "    prec, rec, fscore, _ = precision_recall_fscore_support(y_true_nopad, y_pred_nopad, average = 'weighted')\n",
        "    print('Fold {} Precision: {} Recall: {} F1-Score: {}'.format(fold_num + 1, prec, rec, fscore))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "masking_1 (Masking)          (None, 172)               0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 172, 100)          2211500   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 172, 384)          450048    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 172, 13)           5005      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 172, 13)           0         \n",
            "=================================================================\n",
            "Total params: 2,666,553\n",
            "Trainable params: 2,666,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 14661 samples, validate on 3666 samples\n",
            "Epoch 1/5\n",
            "14661/14661 [==============================] - 65s 4ms/step - loss: 0.3480 - accuracy: 0.9078 - val_loss: 0.2182 - val_accuracy: 0.9200\n",
            "Epoch 2/5\n",
            "14661/14661 [==============================] - 65s 4ms/step - loss: 0.1871 - accuracy: 0.9374 - val_loss: 0.1649 - val_accuracy: 0.9544\n",
            "Epoch 3/5\n",
            "14661/14661 [==============================] - 64s 4ms/step - loss: 0.1083 - accuracy: 0.9697 - val_loss: 0.0757 - val_accuracy: 0.9770\n",
            "Epoch 4/5\n",
            "14661/14661 [==============================] - 63s 4ms/step - loss: 0.0417 - accuracy: 0.9898 - val_loss: 0.0307 - val_accuracy: 0.9927\n",
            "Epoch 5/5\n",
            "14661/14661 [==============================] - 64s 4ms/step - loss: 0.0162 - accuracy: 0.9963 - val_loss: 0.0204 - val_accuracy: 0.9943\n",
            "Fold 1 test_accuracy: 0.9010598534402203\n",
            "Fold 1 Precision: 0.9030468569039867 Recall: 0.9010598534402203 F1-Score: 0.8959284425488834\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "masking_2 (Masking)          (None, 386)               0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 386, 100)          2850100   \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 386, 384)          450048    \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 386, 13)           5005      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 386, 13)           0         \n",
            "=================================================================\n",
            "Total params: 3,305,153\n",
            "Trainable params: 3,305,153\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 14661 samples, validate on 3666 samples\n",
            "Epoch 1/5\n",
            "14661/14661 [==============================] - 142s 10ms/step - loss: 0.2795 - accuracy: 0.9424 - val_loss: 0.1023 - val_accuracy: 0.9643\n",
            "Epoch 2/5\n",
            "14661/14661 [==============================] - 140s 10ms/step - loss: 0.1233 - accuracy: 0.9568 - val_loss: 0.0849 - val_accuracy: 0.9710\n",
            "Epoch 3/5\n",
            "14661/14661 [==============================] - 140s 10ms/step - loss: 0.0823 - accuracy: 0.9782 - val_loss: 0.0459 - val_accuracy: 0.9865\n",
            "Epoch 4/5\n",
            "14661/14661 [==============================] - 140s 10ms/step - loss: 0.0397 - accuracy: 0.9888 - val_loss: 0.0240 - val_accuracy: 0.9935\n",
            "Epoch 5/5\n",
            "14661/14661 [==============================] - 140s 10ms/step - loss: 0.0190 - accuracy: 0.9957 - val_loss: 0.0142 - val_accuracy: 0.9965\n",
            "Fold 2 test_accuracy: 0.9258516345750929\n",
            "Fold 2 Precision: 0.9269133211541687 Recall: 0.9258516345750929 F1-Score: 0.9246340595637227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "masking_3 (Masking)          (None, 386)               0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 386, 100)          2745900   \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 386, 384)          450048    \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 386, 13)           5005      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 386, 13)           0         \n",
            "=================================================================\n",
            "Total params: 3,200,953\n",
            "Trainable params: 3,200,953\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 14662 samples, validate on 3666 samples\n",
            "Epoch 1/5\n",
            "14662/14662 [==============================] - 139s 9ms/step - loss: 0.2624 - accuracy: 0.9421 - val_loss: 0.0914 - val_accuracy: 0.9681\n",
            "Epoch 2/5\n",
            "14662/14662 [==============================] - 138s 9ms/step - loss: 0.1237 - accuracy: 0.9567 - val_loss: 0.0778 - val_accuracy: 0.9730\n",
            "Epoch 3/5\n",
            "14662/14662 [==============================] - 136s 9ms/step - loss: 0.0853 - accuracy: 0.9760 - val_loss: 0.0401 - val_accuracy: 0.9875\n",
            "Epoch 4/5\n",
            "14662/14662 [==============================] - 138s 9ms/step - loss: 0.0361 - accuracy: 0.9904 - val_loss: 0.0186 - val_accuracy: 0.9953\n",
            "Epoch 5/5\n",
            "14662/14662 [==============================] - 136s 9ms/step - loss: 0.0158 - accuracy: 0.9965 - val_loss: 0.0105 - val_accuracy: 0.9972\n",
            "Fold 3 test_accuracy: 0.9274729012801978\n",
            "Fold 3 Precision: 0.9270181956445921 Recall: 0.9274729012801978 F1-Score: 0.925471489906842\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiE8MC8BNeLG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "23009bb4-ff9f-4d5c-c1eb-e501a2059a65"
      },
      "source": [
        "print(\"---Averaged Results over all the epochs---\")\n",
        "test_accuracy = (np.asarray(preds_all_folds) == np.asarray(golds_all_folds)).mean()\n",
        "print('Average K-Fold Test Accuracy: {}'.format(test_accuracy))\n",
        "prec, rec, fscore, _ = precision_recall_fscore_support(preds_all_folds, golds_all_folds, average = 'weighted')\n",
        "print('Average K-Fold Precision: {} Recall: {} F1-Score: {}'.format(prec, rec, fscore))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---Averaged Results over all the epochs---\n",
            "Average K-Fold Test Accuracy: 0.9151568111261053\n",
            "Average K-Fold Precision: 0.9203422617207301 Recall: 0.9151568111261053 F1-Score: 0.9163942865540586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0Tr0kJzNeLO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "d88307ec-bd53-4e76-d213-7a59e9460376"
      },
      "source": [
        "id2tag = {v: k for k, v in tag2id.items()}\n",
        "print(classification_report([id2tag[i] for i in golds_all_folds], [id2tag[i] for i in preds_all_folds]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           .       0.99      1.00      1.00     74779\n",
            "         ADJ       0.84      0.86      0.85     51841\n",
            "         ADP       0.90      0.97      0.94     38739\n",
            "         ADV       0.85      0.72      0.78     37519\n",
            "        CONJ       0.85      0.94      0.89     73239\n",
            "         DET       0.95      0.93      0.94     44180\n",
            "        NOUN       0.91      0.97      0.94     83267\n",
            "         NUM       0.97      0.81      0.88      6794\n",
            "        PRON       0.98      0.81      0.89     27093\n",
            "         PRT       0.95      0.89      0.92     45995\n",
            "        VERB       0.94      0.92      0.93     58401\n",
            "           X       0.00      0.00      0.00       589\n",
            "\n",
            "    accuracy                           0.92    542436\n",
            "   macro avg       0.84      0.82      0.83    542436\n",
            "weighted avg       0.92      0.92      0.91    542436\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0QoOoQQNeLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}