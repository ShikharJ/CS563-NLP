{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries.\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "\n",
    "random.seed(11)\n",
    "np.random.seed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Conv1D, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence(sentence):\n",
    "    '''\n",
    "    Function for parsing the words and tags from the\n",
    "    sentences of the input corpus.\n",
    "    '''\n",
    "    word_tag_pairs = sentence.split(\" \")\n",
    "    words = []\n",
    "    tags = []\n",
    "\n",
    "    for i, word_tag in enumerate(word_tag_pairs):\n",
    "        word, tag = word_tag.strip().rsplit('/', 1)\n",
    "        words.append(word)\n",
    "        tags.append(tag)\n",
    "        \n",
    "    return words, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the sentences into a list.\n",
    "parsed_sentences = []\n",
    "\n",
    "with open('./Brown_train.txt', 'r') as file:\n",
    "    sentences = file.readlines()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        parsed_sentences.append(parse_sentence(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(X_train, Y_train):\n",
    "    '''\n",
    "    Function for building the vocabulary from the training set of\n",
    "    words and tags.\n",
    "    '''\n",
    "    vocabulary2id = dict()    \n",
    "    tag2id = dict()\n",
    "    vocabulary2id['UNK'] = 0\n",
    "    vocabulary2id['PAD'] = 1\n",
    "\n",
    "    for sent in X_train:\n",
    "        for word in sent:\n",
    "            if word not in vocabulary2id.keys():\n",
    "                vocabulary2id[word] = len(vocabulary2id)\n",
    "    \n",
    "    tag2id['PAD'] = 0\n",
    "    for sent in Y_train:\n",
    "        for tag in sent:\n",
    "            if tag not in tag2id.keys():\n",
    "                tag2id[tag] = len(tag2id)\n",
    "    \n",
    "    return vocabulary2id, tag2id\n",
    "\n",
    "def get_word_tag_counts(X_train, Y_train, vocabulary2id, tag2id):\n",
    "    '''\n",
    "    Function for calculating the counts pertaining to the\n",
    "    individual word tags.\n",
    "    '''\n",
    "    wordcount = defaultdict(int)\n",
    "    tagcount = defaultdict(int)\n",
    "    tagpaircount = defaultdict(int)\n",
    "    tagtriplecount = defaultdict(int)\n",
    "    \n",
    "    for sent in X_train:\n",
    "        for word in sent:\n",
    "            wordcount[word] += 1\n",
    "    \n",
    "    for sent in Y_train:\n",
    "        for tag in sent:\n",
    "            tagcount[tag] += 1\n",
    "    \n",
    "    for sent in Y_train:\n",
    "        for i in range(len(sent) - 1):\n",
    "            tagpaircount[sent[i], sent[i + 1]] += 1\n",
    "\n",
    "    for sent in Y_train:\n",
    "        for i in range(len(sent) - 2):\n",
    "            tagtriplecount[sent[i], sent[i + 1], sent[i + 2]] += 1\n",
    "    \n",
    "    return wordcount, tagcount, tagpaircount, tagtriplecount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the test and training sets of sentences.\n",
    "kf = KFold(n_splits = 3, shuffle = False)\n",
    "parsed_sentences = np.asarray(parsed_sentences)\n",
    "scores = []\n",
    "scores1 = []\n",
    "y_pred_idx = []\n",
    "y_pred_idx1 = []\n",
    "y_test_idx = []\n",
    "y_test_idx1 = []\n",
    "\n",
    "preds = []\n",
    "\n",
    "for train_index, test_index in kf.split(parsed_sentences):\n",
    "    train_data = parsed_sentences[train_index]\n",
    "    test_data = parsed_sentences[test_index]\n",
    "    X_train = [a[0] for a in train_data]\n",
    "    Y_train = [a[1] for a in train_data]\n",
    "    X_test = [a[0] for a in test_data]\n",
    "    Y_test = [a[1] for a in test_data]\n",
    "    \n",
    "    # Build the vocabulary and word counts.\n",
    "    vocabulary2id, tag2id = get_vocab(X_train, Y_train)\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padlen = max(len(i) for i in X_train)\n",
    "def pad(sentence, padid=vocabulary2id['PAD']):\n",
    "    out = sentence[:padlen]\n",
    "    padding = [padid for _ in range(padlen - len(out))]\n",
    "    return out + padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ids = np.asarray([pad([vocabulary2id[word] if word in vocabulary2id.keys() else vocabulary2id['UNK'] for word in sent]) for sent in X_train])\n",
    "X_test_ids = np.array([pad([vocabulary2id[word] if word in vocabulary2id.keys() else vocabulary2id['UNK'] for word in sent]) for sent in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_ids = np.asarray([pad([tag2id[word] if word in tag2id.keys() else tag2id['UNK'] for word in sent], tag2id['PAD']) for sent in Y_train])\n",
    "Y_test_ids = np.asarray([pad([tag2id[word] if word in tag2id.keys() else tag2id['UNK'] for word in sent], tag2id['PAD']) for sent in Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2onehot(Y, numtags):\n",
    "    out = []\n",
    "    for s in Y:\n",
    "        categories = []\n",
    "        for item in s:\n",
    "            categories.append(np.zeros(numtags))\n",
    "            categories[-1][item] = 1.0\n",
    "        out.append(categories)\n",
    "    return np.array(out)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_onehot = id2onehot(Y_train_ids, len(tag2id))\n",
    "Y_test_onehot = id2onehot(Y_test_ids, len(tag2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(padlen, )))\n",
    "model.add(Embedding(len(vocabulary2id), 100))\n",
    "model.add(Bidirectional(LSTM(int((128+256)/2), return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2id))))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_ids, Y_train_onehot, batch_size=128, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = np.sum((Y_test_ids == np.argmax(predictions, axis=-1)) * (Y_test_ids != 0)) / np.sum((Y_test_ids != 0))\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_argmax = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nopad = []\n",
    "y_true_nopad = []\n",
    "\n",
    "for i in range(len(Y_test_ids)):\n",
    "    for j in range(len(Y_test_ids[i])):\n",
    "        if Y_test_ids[i][j] != 0 and predictions_argmax[i][j] != 0:\n",
    "            y_true_nopad.append(Y_test_ids[i][j])\n",
    "            if predictions_argmax[i][j] == 0:\n",
    "                y_pred_nopad.append(1)\n",
    "            else:\n",
    "                y_pred_nopad.append(predictions_argmax[i][j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nopad = np.asarray(y_pred_nopad)\n",
    "y_true_nopad = np.asarray(y_true_nopad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_pred_nopad == y_true_nopad).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, rec, fscore, _ = precision_recall_fscore_support(y_true_nopad, y_pred_nopad, average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, rec, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(padlen, )))\n",
    "model.add(Embedding(len(vocabulary2id), 100))\n",
    "model.add(Bidirectional(SimpleRNN(int((128+256)/2), return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2id))))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.003),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_ids, Y_train_onehot, batch_size=128, epochs=4, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_accuracy = np.sum((Y_test_ids == np.argmax(predictions, axis=-1)) * (Y_test_ids != 0)) / np.sum((Y_test_ids != 0))\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_argmax = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nopad = []\n",
    "y_true_nopad = []\n",
    "\n",
    "for i in range(len(Y_test_ids)):\n",
    "    for j in range(len(Y_test_ids[i])):\n",
    "        if Y_test_ids[i][j] != 0 and predictions_argmax[i][j] != 0:\n",
    "            y_true_nopad.append(Y_test_ids[i][j])\n",
    "            if predictions_argmax[i][j] == 0:\n",
    "                y_pred_nopad.append(1)\n",
    "            else:\n",
    "                y_pred_nopad.append(predictions_argmax[i][j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nopad = np.asarray(y_pred_nopad)\n",
    "y_true_nopad = np.asarray(y_true_nopad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_pred_nopad == y_true_nopad).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, rec, fscore, _ = precision_recall_fscore_support(y_true_nopad, y_pred_nopad, average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prec, rec, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true_nopad, y_pred_nopad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
